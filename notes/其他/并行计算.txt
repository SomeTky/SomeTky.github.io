#  第一章 导读

------

## 学习目标

完成本章后，你应该能够：

1. 理解并解释**局部性原理（Locality of Reference）**及其对缓存系统的影响。
2. 掌握**阿姆达尔定律（Amdahl's Law）**并能用它估算系统改进的加速比及其局限。
3. 熟悉**处理器性能方程（CPU performance equation）**及影响性能的三大要素：IC、CPI、时钟周期。
4. 理解可靠性与可用性的基本概念及常用指标：MTTF、MTTR、MTBF 与可用性计算。
5. 认识计算机体系结构与行业发展趋势（芯片工艺、并行/多核、量子计算、智能计算等）。
6. 掌握机器学习相关的基础分类与典型计算平台的概念。

------

## 1. 局部性原理（Principle of Locality of Reference）

### 1.1 概念

- **时间局部性（Temporal Locality）**：最近被访问的数据或指令在不久后仍然有很高概率被再次访问。
- **空间局部性（Spatial Locality）**：与当前被访问位置相邻的地址很可能在接下来被访问（例如数组遍历）。

注：指令局部性通常比数据局部性更强，这也是为什么指令缓存（I-cache）设计非常重要。

### 1.2 为什么重要？

局部性是缓存层次结构（cache hierarchy）设计的理论基础。通过利用局部性，少量快速存储器（cache）就能显著提高程序的平均访问速度。

### 1.3 常见表现形式

- 子程序／函数复用（函数体内执行多次）
- 循环结构（循环体重复执行）
- 顺序访问（扫描数组、读表格）

### 1.4 教学示例

- 想象一个数组的线性扫描：每次访问的下一个元素都在当前地址附近，表现出强烈空间局部性。

------

## 2. 关注常见情况（Focusing on the Common Case）

在系统设计中，应优先优化**发生频率高**的事件。对常见路径的优化会带来对整体性能的最大提升；相反，过度优化罕见情景通常收益甚微。

优化手段举例：并行处理、更优算法、更快的组件、减少 I/O 与通信频率等。

------

## 3. 阿姆达尔定律（Amdahl's Law）

### 3.1 定律表述

若某个任务中比例为 $F_e$ 的部分可以加速，且该部分的加速比为 $S_e$，则总体加速比 $S_n$ 为：

$S_n = \frac{1}{(1 - F_e) + \frac{F_e}{S_e}} $

当 $S_e\rightarrow\infty$（即被加速部分无限快），系统的极限加速为 $1/(1-F_e)$。

### 3.2 启示

- 如果可加速部分 $F_e$ 很小，则总体加速非常受限（收益递减）。
- 投资应优先用于提高耗时多、频繁执行的部分。

### 3.3 数值示例（教材练习）

**题目**：假定为 Web 服务改进 CPU，原 CPU 在计算上忙碌 40%（$F_e=0.4$），I/O 等待占 60%；新 CPU 在计算上比原来快 10 倍（$S_e=10$）。求总体加速比。

**解**：
 $ S_n = \frac{1}{(1 - 0.4) + \frac{0.4}{10}} = \frac{1}{0.6 + 0.04} = \frac{1}{0.64} \approx 1.5625. $

说明：即便计算部分快 10 倍，总体也只提高约 56%——因为 60% 时间花在 I/O 上。

（教材中亦给出当 $S_e=20$ 或 $40$ 时的近似结果，说明收益趋于上限。）

------

## 4. 处理器性能方程（Processor Performance Equation）

处理器执行某程序所需的 CPU 时间可以写为：

$ \text{CPU time} = \text{Instruction Count (IC)} \times \text{CPI} \times \text{clock cycle time} \\ = \frac{IC \times CPI}{\text{clock rate}} $

三个关键因素：

- **IC（指令数）**：程序编译/代码选择影响（CISC / RISC 不同关注点）。
- **CPI（每条指令平均时钟周期数）**：由微架构（流水线、乱序、分支预测等）决定。
- **Clock rate（时钟频率）**：工艺与设计影响。

优化策略示例：减少指令数（编译器优化、算法改进）、降低 CPI（更宽的流水线、更好的预测）、提高时钟频率（更先进工艺），但这些方向互相制约。

------

## 5. 可靠性与可用性（Reliability & Availability）

### 5.1 基本定义

- **MTTF（Mean Time To Failure）**：平均无故障工作时间。
- **MTTR（Mean Time To Repair）**：平均修复时间。
- **MTBF（Mean Time Between Failures）**：通常等于 MTTF + MTTR（在可修复系统中）。

系统可用性（非冗余、有修复）可近似为：
 $ \text{Availability} = \frac{MTTF}{MTTF + MTTR} $

### 5.2 系统故障率组合

若各模块独立、寿命服从指数分布（简化假设），系统整体故障率近似为各模块故障率之和：
 $ \lambda_{system} = \sum_i \lambda_i = \sum_i \frac{1}{MTTF_i} $
 然后系统 MTTF = $1 / \lambda_{system}$。

### 5.3 示例：磁盘子系统（教材示例）

**设定**：

- 10 个磁盘，每个 MTTF = 1,000,000 小时
- 1 个 SCSI 控制器，MTTF = 500,000 小时
- 1 个电源，MTTF = 200,000 小时
- 1 个风扇，MTTF = 200,000 小时
- 1 条 SCSI 电缆，MTTF = 1,000,000 小时

故障率之和：
 $ \lambda = 10/1{,}000{,}000 + 1/500{,}000 + 1/200{,}000 + 1/200{,}000 + 1/1{,}000{,}000. $
 系统 MTTF = $1/\lambda$。 

### 5.4 冗余的作用（双电源示例）

- 两个电源并联可显著提高可靠性。教材给出通过概率与期望值的近似计算，展示双电源比单电源可靠度提升多个数量级的直观结果。

------

## 6. 计算规模单位（大数据与能力量级）

常用单位（以 1024 为进位）示例：

- 1 TB = 1024 GB
- 1 PB = 1024 TB
- 1 EB = 1024 PB
- 1 ZB = 1024 EB
- 1 YB = 1024 ZB

------

## 7. 计算机发展趋势

### 7.1 纵览

- 应用驱动技术演进：不同应用（移动、云、超级计算、嵌入式）推动不同方向的发展。
- 低端：PC 向多核/多线程演进。
- 中端：服务器、SMP（受云、大数据、智能计算驱动）。
- 高端：超级计算机（PFLOPS/EFLOPS 级别）代表国家科研实力。
- 嵌入式与移动设备：IoT、手机、无人系统等市场巨大。

### 7.2 芯片工艺与延迟

摩尔定律推动晶体管密度和性能，但工艺趋向 7nm、5nm、3nm、2nm；在更高频率与更小工艺下，传播延迟（resistance × capacitance）开始占显著份额。

### 7.3 量子计算（简介）

- 处理量子信息的计算模型，基本单位为 qubit（量子比特）。
- 量子计算在某些问题上呈指数性存储或加速潜力。
- 教材援引国内外若干里程碑（示例：113 光子、66 qubit、105 qubit 等实验性原型）作为背景介绍。

------

## 8. 智能时代与 AI 计算平台

### 8.1 智能计算系统的定义

能够高效执行 AI 计算的系统：可包括云集群、超级计算机、SMP、带 GPU 的机器、以及专用加速器（TPU、Ascend、Cambricon 等）。

### 8.2 AI 计算要素

- 专用硬件（AI CPU，TPU，GPU）
- 高性能内存与互连
- 软件栈（框架、编译器、优化库）

### 8.3 机器学习类型回顾

- 监督学习（有标签）
- 无监督学习（无标签）
- 强化学习（基于与环境交互的回报）
- 进化学习（利用启发式/遗传式方法迭代解集）

------

## 9. 小结与复习要点

- **概念词汇**：局部性、CPI、VLSI、MTTF、MTTR、MIPS、FLOPS、Amdahl 定律
- **重要原则**：关注常见情况、局部性是缓存的理论基础、阿姆达尔定律阐明加速的上限
- **实用公式**：Amdahl 定律、CPU 时间方程、可用性公式

------

## 10. 练习题（含答案）

**练习 1（基础）**：某程序在 I/O 上花 30% 时间，计算占 70%。如果将计算部分加速 5 倍，总体加速比是多少？

**答案**：$F_e=0.7,S_e=5$，
 [ S_n = \frac{1}{0.3 + 0.7/5} = \frac{1}{0.3 + 0.14} = \frac{1}{0.44} \approx 2.2727. ]

**练习 2（可靠性）**：三个组件并联（冗余）且独立，当单个组件 MTTF 很大、MTTR 已知时，说明冗余怎样影响系统 MTTF（简要说明，无需精确公式）。

**答案要点**：并联冗余在独立故障假设下把系统失效概率大幅降低，系统平均无故障时间显著增加；但当 MTTR 较长、或冗余切换策略/检测不完善时，实际提升可能低于理论值。



------

# 第二章 高性能内存

> Memory System Architecture & Optimization Techniques
>  内存系统结构与性能优化技术

------

## 📖 2.1 Overview to Memory System

> 内存系统概述

### 💡 Memory System Organization

一个计算机系统主要由以下部分组成：

- **CPU system**（处理系统）
- **Memory system**（存储系统）
- **I/O system**（输入输出系统）
- **Interconnection system**（互连系统）

应用程序对内存的要求：

- **Large capacity**（容量大）
- **High speed**（速度快）
- **Low cost**（成本低）

➡️ 因此，内存通常被设计为**层次结构（Memory Hierarchy）**。

------

## 🏗️ 2.2 Memory Hierarchy 内存层次结构

| 层级                    | 典型设备      | 访问时间       | 容量 | 成本 |
| ----------------------- | ------------- | -------------- | ---- | ---- |
| Register 寄存器         | CPU 内部      | 几百皮秒（ps） | 极小 | 极高 |
| L1/L2/L3 Cache 高速缓存 | 芯片内部/外部 | 纳秒级（ns）   | 小   | 高   |
| Main Memory 主存        | DRAM          | 百纳秒（ns）   | 中   | 中等 |
| Hard Disk 硬盘          | HDD/SSD       | 毫秒级（ms）   | 大   | 低   |

📘 **设计原则：**

- 越靠近 CPU，速度越快、容量越小、成本越高。
- 程序在执行时，大多数访问集中在少量数据区域（**Locality of Reference 局部性原理**）。

------

## ⚙️ 2.3 Cache Organization & Principles

> 缓存结构与基本原理

### 🔍 Locality of Reference 局部性原理

- **Temporal Locality（时间局部性）**：最近使用过的数据可能很快再次使用。
- **Spatial Locality（空间局部性）**：访问一个地址的数据后，可能访问相邻数据。

👉 程序执行中，**90% 的时间消耗在 10% 的代码上**。
 将这 10% 的“热点”代码与数据放入 Cache，可显著提升性能。

------

## 📈 2.4 Cache Performance Metrics

> 缓存性能指标

### 🧮 Average Access Time（平均访问时间）

$$
 T_{avg} = H \times T_1 + (1 - H) \times T_2
$$

其中：

- ( H )：L1 Cache 命中率（Hit Ratio）
- ( T_1 )：L1 Cache 访问时间
- ( T_2 )：L2/主存访问时间

➡️ 当 ( H \to 1 )，则 ( T_{avg} \to T_1 )，即大部分命中时性能最优。

------

### 🧩 Example: Cache Mapping Example

矩阵访问示例分析（4×8 Matrix A）

假设：

- 主存中按列存储（Column-major）
- Cache 有 8 行，每行 2 个字
- 使用 LRU 替换策略
- 访问顺序：`A(0,0)…A(0,7)` → `A(1,0)…A(1,7)`

比较三种映射方式的表现：

| Mapping 类型          | 命中情况 | 替换次数  | 利用率 | 原因分析     |
| --------------------- | -------- | --------- | ------ | ------------ |
| Direct Mapping        | 无命中   | 12 次     | 50%    | 局部性差     |
| Fully Associative     | 8 次命中 | 0 次替换  | 100%   | 最优但成本高 |
| 2-way Set Associative | 无命中   | 12 次替换 | 50%    | 局部性差     |

💭 **改进方法：**
 使用 **Row-major order（按行存储）** 或 **Column-major access（按列访问）**。

------

## 🔢 2.5 Cache Miss Analysis — 4C Model

> 四类缓存未命中模型

| 类型                        | 原因                 | 特点                 | 改进方向       |
| --------------------------- | -------------------- | -------------------- | -------------- |
| Compulsory Miss（强制失效） | 首次访问             | 不可避免             | 增大块大小     |
| Capacity Miss（容量失效）   | Cache 容量不足       | 比例最高（可达 90%） | 扩大容量       |
| Conflict Miss（冲突失效）   | 多块映射同一行       | 可通过高关联度改善   | 提高关联度     |
| Coherency Miss（相干失效）  | 多核系统中数据不一致 | 多 CPU/SMP 特有      | 使用一致性协议 |

🧠 思政补充：第四个 “C”（Coherency）是中国学者扩展提出的。

------

## ⚡ 2.6 Improving Cache Performance

> 缓存性能优化

### 📊 优化方向

1. 减少 **Miss Rate（未命中率）**
   - 增大缓存块（Block Size）
   - 增大缓存容量
   - 提高关联度
   - 改善程序局部性
2. 降低 **Miss Penalty（失效惩罚）**
   - 多级缓存（Multilevel Cache）
   - 读优先策略
3. 降低 **Hit Time（命中延迟）**
   - 避免地址转换
   - 简化访问路径

------

### 📏 Block Size 的影响

- ✅ 增大块大小可减少强制失效（Compulsory Miss）
- ⚠️ 过大块会：
  - 增加失效惩罚（Miss Penalty）
  - 降低缓存行数，引发更多冲突失效

📘 **经验法则**：
 块大小在 **32~64 Bytes** 范围时性能最优。

------

### 🧮 多级缓存 (Multilevel Cache)

平均访问时间公式：

$$
 T_{avg} = TH_1 + R_1 \times (TH_2 + R_2 \times T_{memory})
$$

其中：

- ( TH_1 )：L1 命中时间
- ( R_1 )：L1 未命中率
- ( R_2 )：L2 未命中率

💡 L1 要小而快；L2 要大而高关联度。

------

### 💾 写策略优化（Write Policy）

#### 🧱 Write-through + Write Buffer

- 写入同时更新主存与缓冲区（buffer）
- 可提高写性能，但若读操作发生在未刷新的区域，可能读取错误数据

**解决方法：**

- 在读 Miss 时等待 buffer 清空
- 或允许读 Miss 从 buffer 直接读取

#### 💡 Write-back

- 延迟写入主存，仅在替换时更新
- 搭配写缓冲区进一步加速

------

## 🧠 2.7 Programming Optimization for Cache

> 编程级缓存优化技巧

Cache 对程序员是透明的，但**理解其机制可以优化程序性能**。

### 🔁 1. Loop Interchange（循环置换）

将访问顺序改为与存储顺序一致，提升**空间局部性**。

**Before:**

```c
for(j=0;j<100;j++)
  for(i=0;i<5000;i++)
    x[i][j] = 2*x[i][j];
```

**After:**

```c
for(i=0;i<5000;i++)
  for(j=0;j<100;j++)
    x[i][j] = 2*x[i][j];
```

------

### 🔄 2. Loop Fusion（循环融合）

合并访问相同数组的多个独立循环，提升**时间局部性**。

```c
for(i=0;i<N;i++)
  for(j=0;j<N;j++)
    a[i][j] = 1/(b[i][j]*c[i][j]);

for(i=0;i<N;i++)
  for(j=0;j<N;j++)
    d[i][j] = a[i][j] + c[i][j];
```

➡️ 合并后：

```c
for(i=0;i<N;i++)
  for(j=0;j<N;j++){
    a[i][j] = 1/(b[i][j]*c[i][j]);
    d[i][j] = a[i][j] + c[i][j];
  }
```

------

### 🧩 3. Array Merging（数组合并）

多个具有相同索引的数组可能导致 Cache 冲突，可合并为结构体：

```c
struct merge {
  int val;
  int key;
};
struct merge merged_array[SIZE];
```

------

### 🧮 4. Matrix Deblocking（矩阵分块）

对大矩阵运算，按块（Block）访问以提高空间与时间局部性。

```c
for (jj = 0; jj < N; jj += B)
  for (kk = 0; kk < N; kk += B)
    for (i = 0; i < N; i++)
      for (j = jj; j < min(jj+B, N); j++)
        for (k = kk; k < min(kk+B, N); k++)
          x[i][j] += y[i][k] * z[k][j];
```

------

## 🧮 2.8 Parallel Access Memories

> 并行访问存储系统

### 🔸 Single Bank Multiple Word Memory

一次访问多个字（multi-word）

- 地址高位：选择存储单元
- 地址低位：选择字

### 🔸 Multiple Ports Memory

- **Dual-Port Memory**：两个端口同时读写不同单元
- **Multi-Port Memory**：多端口并行访问，常用于多 CPU 系统

------

## 💽 2.9 RAID — Redundant Array of Independent Disks

> 磁盘阵列技术

RAID 技术通过多磁盘并行与冗余提高**可靠性与性能**。
 具有以下特征：

- 多磁盘构成单一逻辑驱动器
- 使用 **striping（条带化）** 分配数据
- 采用 **parity（校验位）** 保证可恢复性

------

### RAID 0

- 无冗余，仅条带化
- 高速、高容量
- 无容错，任意一盘损坏则全部丢失

------

### RAID 1

- 镜像存储（每个盘有副本）
- 高可靠性与高读性能
- 成本高，写性能略低

------

### RAID 4

- 独立数据盘 + 独立校验盘
- 写入需读旧数据、旧校验再更新 → “写惩罚”严重
- 校验盘成为瓶颈

------

### RAID 5

- 校验信息分布在所有磁盘中
- 避免单盘瓶颈
- 性价比高，是应用最广的 RAID 级别

------

### RAID 6

- 双重校验（P: XOR, Q: Reed-Solomon）
- 可容忍两盘同时损坏
- 写性能较 RAID 5 下降约 30%

------

### RAID 10（RAID 0+1）

- RAID 1 镜像后再条带化
- 高性能 + 高容错
- 成本高，常用于高端服务器

------

### RAID 01

- 先 RAID 0，再镜像
- 容错性低于 RAID10，不推荐使用

------

### Other Combined RAID

- RAID50 = RAID5 + RAID0 → 高可靠、高带宽
- RAID53 = RAID5 + RAID3 → 高速传输应用

------

### RAID 7

- 专有架构（Storage Computer Corp.）
- 集成缓存与智能调度
- 高吞吐与高可靠，但成本极高，非开放标准

------

## 🌐 2.10 Network-based Parallel Storage

> 基于网络的并行存储

### 🧱 SAN (Storage Area Network)

- 专用高速存储网络（FC-SAN、IP-SAN、InfiniBand-SAN）
- 块级访问，容量大、性能高

### 📡 NAS (Network Attached Storage)

- 具备文件管理与通信功能的网络存储设备
- IP 协议通信，安装方便，实时性好

------

### 🧠 Object Storage File System（对象存储）

- 将数据与元数据（属性）组合成“对象”
- 智能分布式结构
- 高并行、高扩展性
- 代表系统：**Panasas**

------

## ⚙️ Panasas 架构

| 类型           | 功能             | 组件                      |
| -------------- | ---------------- | ------------------------- |
| Director Blade | 管理与调度       | 2.4GHz Xeon, 4GB DDR      |
| Storage Blade  | 存储与传输       | 1.2GHz CPU, 512MB–2GB RAM |
| 集成交换机     | 内部通信         | Gigabit Ethernet          |
| 性能           | 每柜可达 30Gbps+ |                           |

采用 **Blade 架构**，自动负载均衡、智能缓存与预取，适用于 HPC（高性能计算）。

------

## 🏁 2.11 Summary

| 模块              | 核心内容                            |
| ----------------- | ----------------------------------- |
| Memory Hierarchy  | 寄存器 → Cache → 主存 → 硬盘        |
| Cache Theory      | 局部性原理、映射、替换、写策略      |
| Cache Performance | 命中率、未命中率、访问时间          |
| Optimization      | 结构优化（容量、关联度） + 编程优化 |
| RAID              | 数据冗余与可靠性架构                |
| Network Storage   | SAN、NAS、Object Storage            |

------







# 第三章 CPU 特殊指令系统

## 学习目标

1. 理解指令系统的基本概念及与软硬件的关系
2. 掌握数据的表示（字符编码、大小端）和移位操作的应用
3. 了解 X86 架构下 SIMD 指令集（MMX、SSE、AVX）的核心功能
4. 能初步理解 SIMD 指令的简单应用场景

## 一、指令系统基础概念

### 1.1 什么是指令系统？

- **定义**：又称 “机器指令集”，是 CPU 能识别和执行的全部机器指令的集合。
- **核心作用**：硬件（CPU）和软件（程序）的 “桥梁”—— 程序员写的代码最终要翻译成指令集中的指令，CPU 才能执行。
- **对系统的影响**：
  - 决定 CPU 的硬件结构和功能（CPU 设计的核心任务就是实现指令集）
  - 影响编程语言的结构（比如 C 语言的某些操作依赖指令集支持）
  - 是 CPU 架构师（硬件）和程序员（软件）的 “共同语言”

### 1.2 指令系统的关键区别：RISC vs CISC

- **CISC（复杂指令集）**：指令多且复杂，支持复杂操作（如直接做乘法），常见于早期 CPU（如 X86 早期）。
- **RISC（精简指令集）**：指令少且简单，只保留核心操作（复杂操作需组合简单指令），效率高，常见于手机 CPU（如 ARM）、服务器 CPU（如 PowerPC）。

### 1.3 指令集设计的核心内容

1. **指令格式设计**：指令的二进制结构（比如操作码、地址码的位数）
2. **指令功能设计**：支持哪些操作（如加减、移位、数据传输）
3. **数据类型支持**：能处理整数、浮点数、字符等哪些类型
4. **寻址方式设计**：CPU 如何找到指令中数据的位置（如内存地址、寄存器）

## 二、数据的表示与存储

### 2.1 字符数据的编码

- 计算机用二进制表示字符，常见两种编码：
  1. **ASCII 码（最常用）**：8 位二进制（实际用 7 位）表示一个字符，需记住 4 个关键值：
     - 字符 “0”：ASCII 值 = 48
     - 大写字母 “A”：ASCII 值 = 65（小写 “a”=97，大小写差 32）
     - 回车（enter）：ASCII 值 = 13
     - 空格：ASCII 值 = 32
  2. **EBCDIC 码（少见）**：8 位二进制，早期 IBM 大型机使用，现在几乎不用。
- 实用技巧：小写字母转大写用 “小写字母 & 0xdf”（二进制 11011111），比如 “b”（ASCII=98）&0xdf=66，对应大写 “B”。

### 2.2 数据的大小端（Endian）

- **问题背景**：多字节数据（如 4 字节整数）在内存中怎么存？（内存按字节地址存储，需确定 “高位字节” 和 “低位字节” 的位置）

#### 1. 两种存储方式（用例子理解：数据 = 12345678，十六进制为 0x12345678）

| 类型                  | 定义（核心区别）   | 内存存储（地址从低到高）  | 适用 CPU            |
| --------------------- | ------------------ | ------------------------- | ------------------- |
| 大端（Big Endian）    | 高位字节存在低地址 | 0x12 → 0x34 → 0x56 → 0x78 | 多数 RISC（如 ARM） |
| 小端（Little Endian） | 低位字节存在低地址 | 0x78 → 0x56 → 0x34 → 0x12 | X86、VAX、Alpha     |

#### 2. 特殊情况：双端支持

- PowerPC、ARM 等 CPU 可通过 “状态寄存器” 设置大小端模式，灵活适配不同场景。

#### 3. 为什么要关注大小端？

- 跨架构通信（如 PC（小端）和嵌入式设备（大端）传数据）会出现 “数据错乱”，需统一端模式；
- 操作多字节数据的单个字节（如提取某个字节的值）时，需知道存储方式。

## 三、移位操作（Shift Operations）

### 3.1 移位的类型

CPU 支持 3 类移位，核心是 “移动二进制位”：

1. **算术移位**：考虑符号位（用于有符号数）
   - 算术左移（左移 1 位 =×2，无溢出时）；
   - 算术右移（右移 1 位 =÷2，截断小数，如 3（11）右移 1 位 = 1（1））。
2. **逻辑移位**：不考虑符号位（用于无符号数）
   - 逻辑左移 = 算术左移（×2）；
   - 逻辑右移：高位补 0（如 8（1000）逻辑右移 1 位 = 4（0100））。
3. **循环移位**：移位后溢出的位补到另一端（如 8 位数据 10000000 循环左移 1 位 = 00000001）。

### 3.2 移位的核心优势

**比普通算术运算（×、÷）快得多**！因为移位是硬件直接操作二进制位，无需复杂计算。

### 3.3 移位的 3 个典型应用

#### 应用 1：提取数据中的特定字段

例：16 位内存字存 2 个字符（每个字符 8 位），如何分别发送给设备？

- 发送左边字符：加载数据→逻辑右移 8 位（去掉右边 8 位）→发送；
- 发送右边字符：加载数据→与 0x00FF（二进制 0000000011111111）做 “与” 操作（保留右边 8 位）→发送。

#### 应用 2：快速实现乘除法

- ×2ⁿ：左移 n 位（如 ×4 = 左移 2 位）；
- ÷2ⁿ：右移 n 位（如 ÷8 = 右移 3 位）；
- 复杂乘法：如 ×10=×(8+2)= 左移 3 位 + 左移 1 位（10=2³+2¹）。

#### 应用 3：判断奇偶数（代码例子）

```c
// 判断数字是奇数还是偶数
int odd_even(int num) {
    if ((num >> 1) << 1 == num)  // 右移1位再左移1位，若等于原数→偶数（二进制末位是0）
        return 0;  // 偶数
    else
        return 1;  // 奇数（末位是1，右移后丢失，左移无法恢复）
}
```

#### 应用 4：交换高低字节（循环移位）

- 8 位循环左移：如 0x1234（16 位）循环左移 8 位→0x3412，直接交换高低字节；
- 用于加密算法、编译器优化等场景。

## 四、X86 SIMD 指令集（重点）

### 4.1 什么是 SIMD？

- **定义**：单指令多数据（Single Instruction Multiple Data），即**一条指令同时处理多个数据**（普通指令一次处理 1 个数据），极大提升并行计算效率。
- **用途**：多媒体处理（视频、音频）、3D 图形、科学计算等需要大量重复计算的场景。

### 4.2 SIMD 的发展历程（X86 架构）

按时间顺序，从简单到复杂，功能不断增强：

| 版本 | 发布时间 | 名称   | 核心特点                                      | 应用场景                          |
| ---- | -------- | ------ | --------------------------------------------- | --------------------------------- |
| V1   | 1996     | MMX    | 57 条指令，64 位数据，支持多字节并行操作      | 早期视频 / 音频处理（如淡入淡出） |
| V2   | 1999     | SSE    | 70 条指令，新增 128 位 XMM 寄存器，支持浮点数 | 3D 图形渲染                       |
| V3   | 2000     | SSE2   | 144 条指令，支持双精度浮点数、128 位整数      | 视频编码、科学计算                |
| V4   | 2004     | SSE3   | 新增 13 条指令，优化数据流动                  | 语音识别、电商数据处理            |
| V5   | 2006     | SSSE3  | 新增 32 条指令，增强整数处理                  | 图像滤波                          |
| V6   | 2008     | SSE4   | 新增 54 条指令（分 4.1/4.2），支持字符串处理  | 8×8 SAD 计算（图像匹配）          |
| V7   | 2011     | AVX    | 200 + 条指令，256 位 YMM 寄存器，浮点性能翻倍 | 深度学习、大规模数据计算          |
| V8   | 2013     | AVX2   | 增强整数处理，支持 256 位整数并行             | 视频编解码、加密                  |
| V9   | 2017     | AVX512 | 512 位寄存器，进一步提升并行度                | 高性能计算（如服务器）            |

### 4.3 关键 SIMD 指令集详解

#### 1. MMX（基础入门）

- **核心功能**：针对多媒体，一次处理多个字节 / 字 / 双字（如 8 个字节、4 个 16 位字）；

- **寄存器**：8 个 64 位寄存器（mm0~mm7）；

- **特殊算术模式**（解决溢出问题）：

  - 循环算术（Wraparound）：溢出时截断进位（如 8 位数据 255+1=0）；
  - 饱和算术（Saturation）：溢出时设为最大 / 最小值（如像素值 0~255，255+1=255，0-1=0）。

- **应用例子**：视频淡入淡出

  

  原理：新像素 = A 像素 × 透明度 + B 像素 ×(1 - 透明度)，MMX 一条指令可并行处理 4 个像素的计算，速度比普通指令快 3 倍。

#### 2. SSE 系列（进阶）

- **SSE2**：最常用的版本，支持 128 位数据（如 2 个双精度浮点数、4 个单精度浮点数），适配 Intel Xeon、Pentium 4 等 CPU，用于视频编码（如 H.264）、科学计算；
- **SSE4**：重点优化字符串处理和图像匹配，比如：
  - `MPSADBW`指令：1 条指令完成 8×8 SAD（绝对误差和）计算（图像匹配的核心操作）；
  - `PHMINPOSUM`指令：1 条指令找到无符号数的最小值及位置。

#### 3. AVX（当前主流，初学者重点关注）

- **核心升级**：256 位 YMM 寄存器（16 个），支持更大量数据并行，新增 200 + 条指令；
- **支持语言 / 工具**：C++（C++11 及以上，需包含头文件`<immintrin.h>`）、汇编、编译器自动矢量化（无需手动写指令）；
- **其他**：华为鲲鹏 CPU 的 NEON 指令集与 AVX 功能类似，可兼容使用。

##### （1）AVX 的数据类型

用`__mXXX`表示向量（多个数据打包成的 “数据块”）：

| 类型      | 存储内容（举例）                   |
| --------- | ---------------------------------- |
| `__m128`  | 4 个单精度浮点数（float）          |
| `__m128d` | 2 个双精度浮点数（double）         |
| `__m128i` | 16 个 8 位整数 / 8 个 16 位整数等  |
| `__m256`  | 8 个单精度浮点数（float）          |
| `__m256d` | 4 个双精度浮点数（double）         |
| `__m256i` | 32 个 8 位整数 / 16 个 16 位整数等 |

##### （2）AVX 函数名格式（看懂函数即可入门）

格式：`_mm<bit_width>_<name>_<data_type>`

- `<bit_width>`：向量位数（256 表示 256 位，空则 128 位）；
- `<name>`：操作类型（如`add`= 加法，`mul`= 乘法，`hadd`= 水平加法）；
- `<data_type>`：数据类型（如`ps`=float，`pd`=double，`epi32`=32 位有符号整数）。

例：

- `_mm256_add_ps`：256 位向量的单精度浮点数加法；
- `_mm256_mul_epi32`：256 位向量的 32 位整数乘法。

##### （3）AVX 简单代码例子（计算向量乘法与减法）

```c
#include <immintrin.h>  // 必须包含的AVX头文件
#include <stdio.h>

int main() {
    // 1. 初始化两个256位双精度向量（各存4个double值）
    __m256d vec1 = _mm256_setr_pd(4.0, 5.0, 13.0, 6.0);  // 顺序：res[0]=4.0, res[1]=5.0...
    __m256d vec2 = _mm256_setr_pd(9.0, 3.0, 6.0, 7.0);

    // 2. 向量乘法：vec3 = vec1 × vec2（4个元素同时相乘）
    __m256d vec3 = _mm256_mul_pd(vec1, vec2);

    // 3. 水平减法：vec1 = vec3 - vec4（按水平方向减，具体逻辑按需调整）
    __m256d vec4 = _mm256_setr_pd(1.0, 1.0, 1.0, 1.0);
    __m256d result = _mm256_hsub_pd(vec3, vec4);

    // 4. 输出结果（将向量指针转为double*，方便访问）
    double* res_ptr = (double*)&result;
    printf("结果：%.1lf %.1lf %.1lf %.1lf\n", 
           res_ptr[0], res_ptr[1], res_ptr[2], res_ptr[3]);
    // 输出：35.0 14.0 77.0 41.0（对应36-1、15-1、78-1、42-1）

    return 0;
}
```







# 第四章 流水线技术（Pipelining）

## 学习目标

1. 理解流水线的核心概念和工作原理（类比生活场景，降低理解门槛）
2. 掌握流水线性能的三大指标（加速比、吞吐量、效率）及计算方法
3. 识别流水线的三类 “冒险”（Hazard），并知道对应的解决办法
4. 了解实际 CPU 中的流水线设计（如鲲鹏）和编程优化技巧

## 1. 流水线基础：从 “单线程” 到 “并行重叠”

### 1.1 什么是流水线？

- **通俗定义**：把一个复杂任务（比如 “执行指令”）拆成多个 “小步骤”（子任务），每个步骤由专门的硬件部件处理，这些部件像工厂流水线一样**同时工作**，让多个任务（指令）的不同步骤 “重叠执行”，从而提高整体速度。
- **核心逻辑**：不是 “一个指令执行完再执行下一个”，而是 “前一个指令做第二步时，后一个指令开始做第一步”。
- **例子**：煮饺子（任务）拆成 “擀皮→放馅→捏合→煮制”4 步，4 个人各负责 1 步，同时处理 4 个饺子，总效率比 1 个人从头做到尾高得多。

### 1.2 指令周期：流水线的 “步骤来源”

指令执行需要分阶段（对应流水线的 “阶段”），每个阶段对应一个硬件部件，比如：

1. 取指（IF）：从内存拿指令（部件：指令缓存）
2. 解码（ID）：翻译指令含义（部件：解码器）
3. 执行（EX）：计算或操作（部件：ALU 算术逻辑单元）
4. 访存（MEM）：读写内存 / 缓存（部件：数据缓存）
5. 写回（WB）：把结果存回寄存器（部件：寄存器文件）

> 关键：每个阶段的执行时间尽量接近（避免 “瓶颈”），每个阶段之间用**寄存器（Latch）** 暂存中间结果。

### 1.3 流水线时空图：直观看 “重叠执行”

时空图是理解流水线的最佳工具，横轴是 “时间（时隙 / 周期）”，纵轴是 “流水线阶段”。

#### 例子：6 阶段流水线处理 9 条指令

- 第一个指令（I1）：需要 6 个周期才能完成（从阶段 1 到阶段 6）
- 从第 2 个周期开始，每个周期新投入一条指令（I2、I3...）
- 从第 6 个周期（即 “5 个时隙后”）开始，每个周期都有一条指令完成（I1、I2...）
- 总耗时：`6（第一个指令的周期） + (9-1)（后续8条指令）= 14个周期`

> 对比非流水线：9 条指令 ×6 周期 = 54 个周期，流水线直接把时间压缩到 14 个周期！

## 2. 流水线性能分析：三大核心指标

### 2.1 先明确两个前提

- **流水线周期（τ）**：每个阶段的执行时间（由最慢的阶段决定，即 “瓶颈阶段”）
- **流水线级数（k）**：任务拆成的子步骤数（比如 6 阶段流水线，k=6）
- **指令总数（n）**：需要处理的任务数量（比如 9 条指令，n=9）

### 2.2 三大性能指标

| 指标             | 定义（通俗版）                          | 计算公式                                                     | 关键结论                                                     |
| ---------------- | --------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **加速比（S）**  | 非流水线耗时 ÷ 流水线耗时（值越大越快） | 非流水线耗时：`n×k×τ`（每条指令 k 个周期）流水线耗时：`(k + n - 1)×τ`（首条 k 周期，后续每条 1 周期）加速比：`S = (n×k) / (k + n - 1)` | 理论上限：当 n 很大时，S≈k（比如 6 级流水线，最快是原来的 6 倍）实际中 S 会低于 k（有瓶颈和冒险） |
| **吞吐量（Tp）** | 单位时间完成的指令数（值越大吞吐越高）  | `Tp = n / [(k + n - 1)×τ]`                                   | 提升关键：减小 τ（优化瓶颈阶段）、合理增加 k                 |
| **效率（E）**    | 流水线的 “利用率”（硬件忙的时间占比）   | 实际加速比 ÷ 理论最大加速比（即 E = S /k）                   | 效率和吞吐量正相关：效率越高，吞吐量越大                     |

### 2.3 实际性能的 “陷阱”：为什么阶段越多不一定越好？

理论上 k 越大，S 越接近 k，但实际中 k 过大反而会变慢，原因有三：

1. **寄存器延迟累积**：每个阶段之间的寄存器（Latch）有延迟，k 越多，总延迟越长，反而拉长单条指令的执行时间。
2. **指令依赖概率增加**：k 越多，同时在流水线中的指令越多，指令之间 “抢数据” 或 “分支跳转” 的冲突概率越高。
3. **控制逻辑复杂**：k 越多，需要更多硬件来协调各阶段，逻辑复杂度增加，反而可能延长流水线周期 τ。

### 2.4 不均衡流水线：“短板决定桶的容量”

如果各阶段执行时间不一样（比如阶段 1 用 2τ，其他阶段用 1τ），则：

- 流水线周期 τ 由最慢的阶段决定（这里 τ=2τ）
- 加速比上限：`S_max ≈ （平均阶段时间） / （最慢阶段时间）`

> 解决办法：拆分瓶颈阶段（比如把 2τ 的阶段拆成两个 1τ 的阶段），或复制瓶颈部件（并行处理）。

## 3. 流水线的 “敌人”：三类冒险（Hazard）及解决办法

“冒险” 是指流水线无法正常重叠执行，导致 “气泡（Bubble，空等周期）”，降低效率。主要分三类：

### 3.1 结构冒险（资源冲突）

#### 问题：两个指令 “抢同一个硬件”

- 例子：两条指令同时需要 ALU（算术单元），或同时读写数据缓存。
- 原因：硬件资源不足（比如只有一个 ALU，或缓存未完全流水线化）。

#### 解决办法：

1. **插入气泡**：让后一条指令等 1 个周期（暂时空转）。
2. **增加资源**：复制瓶颈部件（比如两个 ALU，分别处理不同指令）。
3. **预取数据**：提前把指令 / 数据读到缓存，避免访存冲突。

### 3.2 数据冒险（数据依赖）

#### 问题：后一条指令需要前一条指令的结果，但前一条还没算完

- 例子：

  

  ```
  ADD R1, R2, R3
  ```

  （计算 R1=R2+R3，还没写回 R1）

  

  ```
  SUB R4, R1, R5
  ```

  （需要读 R1，但 R1 还没更新）

#### 三种数据冒险类型（记缩写更简单）

| 类型 | 英文              | 通俗解释                     | 例子                                                 |
| ---- | ----------------- | ---------------------------- | ---------------------------------------------------- |
| RAW  | Read After Write  | 读在写之前（真依赖，最常见） | ADD 写 R1 → SUB 读 R1（SUB 读早了）                  |
| WAR  | Write After Read  | 写在读之前（名依赖，假依赖） | SUB 读 R1 → ADD 写 R1（ADD 写早了）                  |
| WAW  | Write After Write | 写在写之前（名依赖，假依赖） | SUB 写 R1 → ADD 写 R1（ADD 写早了，覆盖 SUB 的结果） |

#### 解决办法：

1. **转发（Forwarding/Bypassing）**：直接把前一条指令的结果 “绕过寄存器” 传给后一条指令的硬件（只解决 RAW）。

   

   例子：ADD 在执行阶段（EX）算出 R1 的结果后，直接传给 SUB 的 EX 阶段，不用等 ADD 写回寄存器（WB 阶段）。

2. **寄存器重命名**：把 “名依赖” 的寄存器换成临时寄存器（解决 WAR 和 WAW）。

   

   例子：把 ADD 的目标寄存器从 R1 换成临时寄存器 T1，SUB 读 R1，ADD 写 T1，互不冲突。

3. **插入气泡**：如果转发解决不了（比如数据还在内存中），就让后一条指令空等 1-2 个周期。

### 3.3 控制冒险（分支冲突）

#### 问题：遇到 “分支指令”（比如 if-else、for 循环），流水线不知道下一步该取哪条指令

- 例子：`if (a>0) 跳转到Label1`，流水线已经取了 “Label1 之后” 的指令，但实际 a≤0，这些指令白取了，需要清空，导致 “分支惩罚”。

#### 分支惩罚的影响：

- 对于 k 级流水线，最坏情况惩罚是（k-1）个周期（比如 6 级流水线，惩罚 5 个周期）。
- 实际中：程序里约 20% 是分支指令，60% 的分支会跳转，8 级流水线的吞吐量会下降 46%！

#### 解决办法：

核心思路：**减少 “猜错” 的概率**或**利用延迟槽填无关指令**。

| 方法             | 原理                                                         | 适用场景                       |
| ---------------- | ------------------------------------------------------------ | ------------------------------ |
| **静态分支预测** | 不看历史，固定猜一种（比如 “永远不跳转” 或 “永远跳转”）      | 简单硬件，低开销               |
| **动态分支预测** | 看历史记录（比如用 2 位计数器记录 “最近跳没跳”），猜下次是否跳转 | 复杂硬件，高准确率（82%-99%）  |
| **延迟分支**     | 编译器调整指令顺序，把 “无关指令” 填到分支指令后面的 “延迟槽”，避免气泡 | RISC CPU（如 ARM），依赖编译器 |

> 动态分支预测细节：用 “分支目标缓冲区（BTB）” 存分支地址和历史，2 位计数器分 4 种状态：强跳转、弱跳转、强不跳转、弱不跳转，根据历史动态调整。

## 4. 流水线优化技巧：从硬件到软件

### 4.1 循环展开（Loop Unrolling）：减少分支开销

#### 问题：for 循环的 “判断条件”（比如 i--; if (i>0) 继续）是分支指令，每次循环都有分支惩罚。

#### 解决办法：把循环体 “复制多份”，减少循环次数。

- 例子：原循环（i 从 1000 减到 1，每次处理 1 个元素）

  

  ```
  for(i=1000; i>0; i--) x[i] = x[i] + s;
  ```

- 展开 4 次后（每次处理 4 个元素，循环次数从 1000→250）：

  

  ```
  for(i=1000; i>0; i-=4) {
  ```

  

  ```
  x[i] = x[i] + s;
  ```

  

  ```
  x[i-1] = x[i-1] + s;
  ```

  

  ```
  x[i-2] = x[i-2] + s;
  ```

  

  ```
  x[i-3] = x[i-3] + s;
  ```

  

  ```
  }
  ```

#### 优缺点：

- 优点：减少分支次数（从 1000 次→250 次），提高流水线利用率。
- 缺点：占用更多寄存器（要暂存多个元素），可能增加指令缓存（I-cache）的缺失率。

### 4.2 寄存器重命名：解决名依赖

- 作用：把 WAR/WAW 中的 “冲突寄存器” 换成临时寄存器，让指令可以 “乱序执行”（Out-of-Order Execution）。

- 例子：原指令（有 WAW 冲突）

  

  ```
  SUB R1, R4, R3
  ```

  （写 R1）

  

  ```
  ADD R1, R2, R3
  ```

  （也写 R1）

- 重命名后：

  

  ```
  SUB T1, R4, R3
  ```

  （写临时寄存器 T1）

  

  ```
  ADD R1, R2, R3
  ```

  （写 R1）

  

  （后续用 R1 和 T1 的地方按需调用，无冲突）

### 4.3 动态调度：硬件 “智能排序” 指令

- 静态调度：编译器排好指令顺序，遇到依赖就插入气泡。
- 动态调度：硬件根据实时的指令依赖关系，调整指令执行顺序（比如把不依赖的指令提前执行），减少气泡。
- 适用场景：复杂 CPU（如 x86、ARM），能处理编译器无法预判的依赖。

## 5. 实际应用：从 CPU 到编程

### 5.1 鲲鹏 CPU 的流水线设计（基于 ARM v8）

鲲鹏作为实际 CPU，流水线分多个并行子流水线，适配不同指令类型：

1. **子流水线类型**：整数流水线、浮点流水线、加载存储流水线、分支预测流水线。

2. **核心阶段（7 个）**：

   

   取指（IF）→ 解码（ID）→ 分配（Allocation，寄存器重命名 + 资源预留）→ 发射（Issue，分发给执行单元）→ 执行（EX）→ 写回（WB）→ 提交（Commit，确认结果正确）。

3. **亮点**：支持 128 位浮点指令（一次处理更多数据）、SIMD（单指令多数据），提升并行效率。

### 5.2 编程中的流水线优化技巧（初学者可落地）

#### 技巧 1：减少循环内的条件判断

- 问题：循环内的 if-else 会引发控制冒险。

- 解决：把 “条件判断” 提到循环外，分两个循环处理。

  

  例子：原代码（循环内有 if）

  

  ```
  for(i=1; i<k; i++) {
  ```

  

  ```
  if(a==0) f[i] = 1.0;
  ```

  

  ```
  else f[i] = sin(a*i);
  ```

  

  ```
  }
  ```

  

  优化后：

  

  ```
  if(a==0) {
  ```

  

  ```
  for(i=1; i<k; i++) f[i] = 1.0;
  ```

  

  ```
  } else {
  ```

  

  ```
  for(i=1; i<k; i++) f[i] = sin(a*i);
  ```

  

  ```
  }
  ```

#### 技巧 2：循环倒置（Loop Inverting）

- 问题：while 循环需要 “先判断再执行”，多一次分支。

- 解决：改成 do-while 循环（先执行再判断），减少一次分支。

  

  例子：原 while 循环

  

  ```
  i=0; while(i<100) { 处理; i++; }
  ```

  

  优化后 do-while 循环

  

  ```
  i=0; do { 处理; i++; } while(i<100);
  ```

#### 技巧 3：分支规避（Branch Avoidance）

- 用 “三目运算符”“查表法” 代替 if-else，避免分支。

  

  例子：

  ```
  if(c) x=y; else x=z;
  ```

   

  → 优化为

   

  ```
  x = c ? y : z;
  ```

  

  （三目运算符由硬件直接处理，无分支惩罚）

## 6. 总结与复习要点

### 核心逻辑

流水线的本质是 “**重叠执行**”，通过拆分任务、并行处理子步骤，提升整体效率。

### 关键知识点

1. **性能指标**：加速比（理论上限 k，实际低于 k）、吞吐量（单位时间指令数）、效率（硬件利用率）。
2. **三类冒险**：
   - 结构冒险：抢资源→加硬件 / 插气泡；
   - 数据冒险：抢数据→转发 / 重命名 / 插气泡；
   - 控制冒险：分支跳转→预测 / 延迟分支。
3. **优化技巧**：循环展开（减分支）、寄存器重命名（解名依赖）、动态调度（硬件排序）。

### 初学者常见误区

- 误区 1：流水线级数越多越好→实际中 k 过大有延迟和依赖问题；
- 误区 2：只要用流水线就一定快→忽略冒险和瓶颈的影响；
- 误区 3：优化是硬件的事→编程时减少分支、循环展开也能提升流水线效率。







# 第五章 并行计算架构与编程

## 一、多核计算基础

### 1.1 什么是多核（CMP）

- **定义**：多核（Chip Multiprocessor，CMP）是将 2 个及以上独立处理器（称为 “核心”）集成在同一块硅片（die）上的架构。
- **核心组成**：每个核心包含独立处理器的所有组件 —— 寄存器、超标量执行单元、控制单元（CU）、多级缓存（L1、L2 等）。
- **为什么需要多核**：
  1. **功耗与散热需求**：单核 CPU 频率提升会导致功耗和散热急剧增加，多核通过 “并行而非高频” 提升性能，更节能。
  2. **并行性需求**：传统单核依赖 “指令级并行（ILP）”，提升空间有限；多核转向 “线程级并行（TLP）”，通过多线程同时执行提升效率。
  3. **摩尔定律延续**：晶体管数量增长不再用于提升单核频率，而是增加核心数。

### 1.2 多核架构关键组件

#### （1）缓存结构：共享缓存的优势

多核缓存通常分为 “私有缓存”（如每个核心的 L1）和 “共享缓存”（如多个核心共享的 L2/L3），共享 L2 缓存的核心优势：

- **减少缓存缺失**：一个核心加载的内存块存入共享缓存后，其他核心访问同一数据时直接命中，无需重复读取主存。
- **避免数据冗余**：共享数据在共享缓存层不重复存储，动态分配缓存空间（局部性差的线程可占用更多缓存）。
- **简化通信与一致性**：核心间可通过共享缓存通信，且缓存一致性问题仅需处理 L1 私有缓存（降低复杂度）。

#### （2）核心互连方式

互连是连接多个核心与共享缓存的硬件机制，直接影响性能，常见类型：

| 互连方式             | 特点                                             | 应用案例                      |
| -------------------- | ------------------------------------------------ | ----------------------------- |
| 总线（Bus）          | 结构简单、成本低，但多核心竞争时会拥堵           | Intel Core Duo、Core i7       |
| 交叉开关（Crossbar） | 多输入直接连接多输出，带宽高、无拥堵             | AMD64 Opteron、Athlon X2      |
| 环形（Ring）         | 核心连成闭环，分布式控制（基于令牌），复杂度中等 | IBM Power4/5、鲲鹏 920        |
| 片上网络（NoC）      | 类似计算机网络，通过路由器转发数据，支持大量核心 | 多核 / 众核架构（如大型 GPU） |

#### （3）缓存一致性：MESI 协议

多个核心的私有缓存可能存储同一数据，需通过协议保证数据一致，最常用的是**MESI 协议**（4 种状态）：

- **M（Modified，修改）**：缓存块被修改，与主存不一致，仅当前核心持有。
- **E（Exclusive，独占）**：缓存块与主存一致，仅当前核心持有，可直接修改为 M 状态。
- **S（Shared，共享）**：缓存块与主存一致，多个核心持有，修改前需先变为独占。
- **I（Invalid，无效）**：缓存块失效，需重新从主存或共享缓存加载。

#### （4）共享缓存替换算法：ATR

当共享缓存满时，需替换旧缓存块，**自适应计时替换（ATR）** 比传统 LRU（最近最少使用）更优：

- **核心思想**：给高优先级、高局部性的线程 / 数据分配更长的 “存活时间”，避免有用数据被误替换。
- **实现逻辑**：
  1. 每个缓存块关联一个 “衰减计数器”，定期递减。
  2. 高优先级线程的缓存块衰减速度更慢（衰减间隔长）。
  3. 缓存块被访问时，计数器重置；计数器为 0 时成为替换候选。

### 1.3 实用案例：鲲鹏 920 CPU

- **核心规格**：最多 64 核，7nm 工艺，主频最高 3GHz。
- **缓存配置**：每核 L1（64K 指令 + 64K 数据）、L2（512K），共享 L3（64MB）。
- **架构特点**：采用 “环形互连”，CPU 由 3 个 die 组成，每个 die 包含 6-8 个集群（1 个集群 = 4 核 + 共享 L3），支持 SIMD 指令加速。

## 二、GPU 架构基础

### 2.1 GPU 与 CPU 的核心区别

GPU（图形处理器）专为并行计算设计，与 CPU 差异显著：

| 特性     | CPU                | GPU                                         |
| -------- | ------------------ | ------------------------------------------- |
| 核心数量 | 少（通常 2-64 核） | 极多（数千核）                              |
| ALU 占比 | 5%（侧重控制逻辑） | 40%（侧重计算）                             |
| 并行模型 | 多线程（TLP）      | 单指令多线程（SIMT）                        |
| 适用场景 | 通用计算、复杂控制 | 图形渲染、大规模并行计算（如 AI、科学计算） |

### 2.2 GPU 核心组件

- **SP（Streaming Processor，流处理器）**：GPU 的基本计算单元，执行简单算术运算。
- **SM（Streaming Multiprocessor，流多处理器）**：由多个 SP + 特殊功能单元（SFU，如计算 sin、log）组成，是 GPU 的核心计算模块。
- **TPC（Texture Processor Cluster，纹理处理器集群）**：包含多个 SM 和纹理单元，处理图形中的纹理映射。
- **SIMT（Single Instruction Multiple Threads）**：一个指令同时控制多个线程执行，线程按 “warp（ warp 大小通常为 32）” 分组，硬件自动处理分支 divergence（分歧）。

### 2.3 GPU 编程模型

GPU 编程需依托专用模型，常用模型对比：

| 编程模型 | 开发者     | 特点                                    | 适用场景                      |
| -------- | ---------- | --------------------------------------- | ----------------------------- |
| CUDA     | NVIDIA     | 专有模型，易用性高，深度优化 NVIDIA GPU | NVIDIA GPU 开发（如 AI 训练） |
| OpenCL   | 跨行业联盟 | 开源跨平台，支持 CPU/GPU/FPGA           | 多厂商硬件兼容开发            |
| C++ AMP  | Microsoft  | 高抽象度，基于 C++ 扩展                 | Windows 平台并行计算          |
| OpenACC  | 跨行业联盟 | 类似 OpenMP，自动并行化串行代码         | 快速将串行代码迁移到 GPU      |

## 三、并行编程理论基础

### 3.1 两大核心定律

#### （1）Amdahl 定律：固定问题规模下的加速比

- **核心思想**：程序的加速比由 “串行部分占比” 决定，无法通过无限增加核心数突破上限。

- **公式**：\(S_n = \frac{1}{S + \frac{1-S}{n} + H(n)}\)

  

  其中：

  S

  = 串行部分占比，

  n

  = 核心数，

  \(H(n)\)

  = 线程开销（调度、同步等）。

- **结论**：

  1. 串行部分占比越低，加速比越高（如\(S=0.05\)时，32 核加速比约 16；\(S=0.3\)时，32 核加速比仅 3）。
  2. 线程开销\(H(n)\)过大会导致加速比下降，甚至低于串行（\(S_n<1\)），需尽量减少。

#### （2）Gustafson 定律：可变问题规模下的加速比

- **核心思想**：当问题规模随核心数增加而扩大时，串行部分占比相对降低，加速比可随核心数线性增长。

- **公式**：\(Scaled\ Speedup = n + (1-n)S\)

  

  其中：

  n

  = 核心数，

  S

  = 串行部分占比。

- **适用场景**：Web 服务、分布式数据库、AI 训练等 “问题规模可扩展” 的场景。

### 3.2 进程与线程的区别

进程和线程是并行计算的基本执行单元，初学者需明确差异：

| 对比维度 | 进程（Process）                | 线程（Thread）                        |
| -------- | ------------------------------ | ------------------------------------- |
| 地址空间 | 独立（每个进程有专属地址空间） | 共享（同一进程内线程共享地址空间）    |
| 粒度     | 粗粒度（资源占用多）           | 细粒度（资源占用少）                  |
| 调度开销 | 大（切换需保存完整上下文）     | 小（仅需保存寄存器和栈，0.1-10 周期） |
| 资源共享 | 需通过 IPC（如管道、消息队列） | 直接共享进程资源（如内存、文件句柄）  |
| 管理结构 | 进程控制块（PCB，体积大）      | 线程控制块（TCB，体积小）             |

### 3.3 线程层级

线程从抽象到硬件执行分为 3 层，逐层映射：

1. **用户级线程**：由应用程序创建（如 Java 的 Thread），对内核不可见。

2. **内核级线程**：由操作系统管理（如 Linux 的 pthread），是调度的基本单位。

3. **硬件级线程**：由 CPU 核心执行（如超线程技术、多核），是硬件执行单位。

   

   映射关系

   ：用户级线程 → 内核级线程 → 硬件级线程（OS 和运行时环境自动处理，对程序员透明）。

## 四、并行编程核心概念

### 4.1 分解：并行的基础

分解是将程序拆分为可并行执行的 “任务” 或 “数据块”，核心分为两类：

| 分解类型 | 定义                                                         | 适用场景                                   |
| -------- | ------------------------------------------------------------ | ------------------------------------------ |
| 功能分解 | 按 “功能模块” 拆分任务（如一个任务处理输入，一个处理计算）   | 任务逻辑独立的场景（如流水线）             |
| 数据分解 | 按 “数据” 拆分，同一计算逻辑处理不同数据块（如分块矩阵乘法） | 数据量大、计算逻辑重复的场景（如 AI 推理） |

**分解要点**：

- 任务 / 数据块需尽量独立，减少依赖（降低同步开销）。
- 任务粒度适中：太小会增加管理开销，太大则无法充分利用多核。

### 4.2 同步：解决依赖与竞争

#### （1）为什么需要同步？

1. **数据依赖**：任务 A 的输出是任务 B 的输入，需保证 A 执行完再执行 B（功能依赖）。
2. **资源竞争**：多个任务同时访问同一共享资源（如共享变量），需避免 “race condition（竞态条件）”。

#### （2）常用同步机制

- **锁（Lock）**：保证同一时间只有一个任务访问共享资源（如互斥锁 mutex）。
- **信号量（Semaphore）**：控制同时访问资源的任务数量（如限制 5 个任务同时读写文件）。
- **屏障（Barrier）**：所有任务到达屏障后才继续执行（如并行计算中，所有分块计算完再汇总）。
- **临界区（Critical Section）**：用同步机制保护的代码段，仅允许一个任务进入。

### 4.3 线程间通信

通信是线程交换数据的方式，核心分为两类：

| 通信方式     | 定义                                                        | 特点                                     |
| ------------ | ----------------------------------------------------------- | ---------------------------------------- |
| 共享内存通信 | 线程通过读写 “共享内存”（如共享变量、缓存）交换数据         | 速度快，但需同步保护                     |
| 消息传递通信 | 线程通过 “发送 / 接收消息” 交换数据（如 MPI_Send/MPI_Recv） | 无共享资源竞争，适合分布式系统（如集群） |

**消息传递模式**：单播（一对一）、广播（一对多）、汇聚（多对一，如求和）、全交换（多对多）。

## 五、并行程序设计模式

并行程序设计需遵循 “四步流程”，帮助初学者系统化设计：

### 5.1 第一步：找并发

分析程序是否适合并行，核心三步骤：

1. **问题分解**：拆分为任务 / 数据块（功能或数据分解）。
2. **依赖分析**：识别任务间的依赖（如数据依赖、时间依赖），将无依赖的任务分组。
3. **设计评估**：判断设计是否适配目标平台（如多核 / 集群）、是否简单高效。

### 5.2 第二步：算法结构设计

将并发映射到线程 / 进程，按 3 种方式组织：

| 组织方式     | 适用场景                                 | 典型模式                             |
| ------------ | ---------------------------------------- | ------------------------------------ |
| 按任务组织   | 任务逻辑独立（如输入、计算、输出分离）   | 任务并行、分治（Divide and Conquer） |
| 按数据组织   | 数据分块，计算逻辑重复（如分块矩阵）     | 几何分解、递归数据分解               |
| 按数据流组织 | 数据按流水线传递（如先滤波再傅里叶变换） | 流水线（Pipeline）、事件驱动         |

### 5.3 第三步：支持结构设计

#### （1）程序结构：代码组织方式

| 程序结构                     | 定义                                                         | 适用场景                     |
| ---------------------------- | ------------------------------------------------------------ | ---------------------------- |
| SPMD（单程序多数据）         | 所有线程执行同一程序，但处理不同数据                         | 最常用（如 CUDA、OpenMP）    |
| Master/Worker（主从）        | 主线程分配任务，工作线程执行任务                             | 动态任务分配（如任务队列）   |
| 循环并行（Loop Parallelism） | 将串行循环改为并行循环（如 OpenMP 的`#pragma omp parallel for`） | 循环密集型程序（如数值计算） |
| Fork/Join（分叉 / 合并）     | 主线程分叉出子线程并行执行，子线程执行完后合并               | 动态任务（如递归分治）       |

#### （2）数据结构：适配并行的存储方式

- **共享数据**：封装为抽象数据类型（ADT），用同步控制访问（如线程安全的队列）。
- **共享队列**：用于 Master/Worker 模式，主线程放任务，工作线程取任务。
- **分布式数组**：将大数组分块存储（如 1D 按行分、2D 按子块分），减少跨核心数据访问。

### 5.4 第四步：实现机制

- **线程管理**：创建、销毁线程，设置线程优先级（如 Java 的`Thread.start()`、Linux 的`pthread_create()`）。
- **同步与通信**：调用 API 实现同步（如`pthread_mutex_lock()`）和通信（如 MPI 的`MPI_Recv()`）。

## 六、并行编程常用 API

初学者可从以下 API 入手，覆盖不同平台和场景：

### 6.1 操作系统级 API

| API 类型       | 平台       | 特点                                     | 关键函数 / 组件                            |
| -------------- | ---------- | ---------------------------------------- | ------------------------------------------ |
| Win32 API      | Windows    | 底层 API，支持线程创建、同步、亲和性设置 | `CreateThread()`、`WaitForSingleObject()`  |
| MFC            | Windows    | 封装 Win32，支持线程池                   | `CWinThread`类                             |
| .NET Threading | Windows    | 高抽象度，支持任务并行库（TPL）          | `System.Threading.Thread`、`Task`          |
| Pthreads       | Linux/Unix | 跨平台线程库，C 语言接口                 | `pthread_create()`、`pthread_mutex_init()` |

### 6.2 并行编程专用 API

#### （1）OpenMP：共享内存并行（入门首选）

- **特点**：基于 “编译制导指令”（如`#pragma omp parallel for`），无需修改大量代码，快速并行化串行程序。

- **执行模型**：Fork-Join（主线程分叉出子线程执行并行任务，执行完后合并）。

- **简单示例**（并行循环）：

  c

  

  运行

  

  

  

  

  ```c
  #include <omp.h>
  int main() {
    int sum = 0;
    // 并行执行循环，每个线程私有变量i
    #pragma omp parallel for private(i) reduction(+:sum)
    for (int i = 0; i < 100; i++) {
      sum += i;
    }
    return 0;
  }
  ```

  

#### （2）MPI：消息传递并行（分布式系统）

- **特点**：用于集群、SMP 等分布式平台，以 “进程” 为执行单元，通过消息传递通信。
- **开源实现**：MPICH、LAM MPI（可在 Linux/Windows 上运行）。
- **核心功能**：消息发送（`MPI_Send`）、接收（`MPI_Recv`）、汇聚（`MPI_Reduce`）。

## 七、学习重点总结

1. **架构层面**：理解多核的缓存结构、互连方式、MESI 协议；GPU 的 SIMT 模型和核心组件。
2. **理论层面**：掌握 Amdahl 定律（固定问题）和 Gustafson 定律（可变问题），明确并行的上限和优化方向。
3. **编程层面**：
   - 学会 “分解”（功能 / 数据分解）和 “同步”（锁、屏障）。
   - 入门 API：先用 OpenMP 实现共享内存并行，再尝试 MPI 实现分布式并行。
4. **实践层面**：结合案例（如鲲鹏 920、NVIDIA GPU）理解理论，通过简单代码（如并行求和、矩阵乘法）巩固知识点。







# 第六章 深度学习处理器与编程框架

## 一、引言

本章聚焦**深度学习处理器（DLP）** 和**深度学习编程框架**（如 TensorFlow、MindSpore），核心解决两个问题：

1. 为什么需要专门的深度学习处理器？
2. 如何通过编程框架快速实现深度学习模型？

## 二、机器学习算法概览

深度学习是机器学习的一个分支，先了解机器学习的主要算法分类（简化版，聚焦核心方向）：

| 算法大类         | 典型代表                                | 用途场景                      |
| ---------------- | --------------------------------------- | ----------------------------- |
| 贝叶斯算法       | 朴素贝叶斯、贝叶斯信念网络（BBN）       | 文本分类、垃圾邮件识别        |
| 决策树与集成学习 | 决策树（CART、ID3）、随机森林、AdaBoost | 分类回归、特征重要性分析      |
| 神经网络         | 感知机、Hopfield 网络                   | 简单分类、联想记忆            |
| 深度学习         | CNN（卷积神经网络）、Transformer        | 图像识别、自然语言处理（NLP） |
| 降维与回归       | PCA（主成分分析）、线性回归、LASSO      | 数据压缩、预测（如房价）      |
| 聚类算法         | k-Means、层次聚类                       | 无标签数据分组（如用户分群）  |

## 三、为什么需要深度学习处理器（DLP）？

普通 CPU/GPU 处理深度学习模型存在**性价比低**的问题，原因如下：

1. 深度学习模型规模爆炸：
   - ResNet 有 152 层，商业模型甚至有 512 层、110 亿参数；
   - 谷歌 Switch Transformer 参数达 1.6 万亿，普通硬件无法高效承载。
2. 普通硬件效率低：
   - 谷歌用 1.6 万个 CPU 核跑 7 天才能完成猫脸识别；
   - AlphaGo 对战李世石需 1000 个 CPU + 200 个 GPU。

### 深度学习的核心特点（决定了 DLP 的设计方向）

- **层数多、种类少**：核心层只有 3 种 —— 卷积层（提取特征）、池化层（压缩数据）、全连接层（输出结果）；
- **计算多、控制少**：无复杂逻辑判断，大量重复计算（如矩阵乘法），适合并行；
- **数据局部性强**：卷积核（权重）可重复使用，减少内存读写开销。

## 四、深度学习处理器（DLP）核心知识

### 1. DLP 的核心计算单元

DLP 的设计完全匹配深度学习的计算需求，核心单元包括：

- **MAC 单元**：乘法累加器（Multiply and Accumulator），负责深度学习中最核心的 “权重 × 输入 + 累加” 计算（如 `c = a1*b1 + a2*b2 + ... + an*bn`）；
- **向量 / 矩阵函数单元**：处理批量数据，比如一次性完成 16×16 矩阵乘法；
- **激活函数单元**：计算 ReLU、Sigmoid 等激活函数，引入非线性特征；
- **池化单元**：执行最大池化（max-pool）、平均池化（avg-pool），压缩特征图。

### 2. DLP 的典型架构（简化版）

DLP 的架构围绕 “高效并行计算” 设计，核心模块：

- **控制处理器（CP）**：指挥全局，分配任务；
- **神经元功能部件（NFU）**：执行具体的计算（如卷积、激活）；
- **Scratchpad 内存**：高速 SRAM，存放临时数据（如特征图、权重），比普通内存快得多；
- **神经元选择模块（NSM）**：处理数据稀疏性，跳过无效计算（如输入为 0 的神经元）；
- **处理单元（PE）**：包含 MAC 和选择模块，是并行计算的基本单元。

### 3. 实例：华为 Ascend 910

- 工艺：7nm 先进制程；
- 核心：Da Vinci AI 核心（专门优化矩阵计算）；
- 内存：4 个 HBM2.0 显存，共 32GB，高带宽低延迟；
- 用途：支持超大规模模型（如千亿参数模型），用于云服务器和 AI 训练。

## 五、深度学习编程框架：让开发更简单

### 1. 编程框架的作用

把深度学习中的复杂操作（如卷积、反向传播）封装成 “组件”，开发者无需手写底层计算，只需调用接口即可搭建模型。

### 2. 主流框架对比

| 框架       | 开发者   | 核心特点                        | 适用场景                   |
| ---------- | -------- | ------------------------------- | -------------------------- |
| TensorFlow | 谷歌     | 支持多语言、多设备，生态最完善  | 科研、工业界（图像、NLP）  |
| MindSpore  | 华为     | 全场景统一 API，自动并行 / 微分 | 端边云协同（手机、服务器） |
| Caffe      | 伯克利   | 基于 “层” 设计，适合图像分类    | 传统图像任务               |
| PyTorch    | Facebook | 动态图优先，调试方便            | 科研、快速原型开发         |

### 3. 编程接口的两种类型

#### （1）基于数据流图（如 TensorFlow）

- **概念**：用 “有向图” 描述计算过程 ——

  - 节点（Node）：表示操作（如加法、卷积）；
  - 边（Edge）：表示数据（即 “张量”，Tensor）。

- **例子**：计算 `y = w × x + b` 的数据流图：

  

  ```
  x
  ```

  （输入）→ [matmul（矩阵乘法）] ←

   

  ```
  w
  ```

  （权重）→ [add（加法）] ←

   

  ```
  b
  ```

  （偏置）→

   

  ```
  y
  ```

  （输出）。

#### （2）基于层（如 Caffe）

- **概念**：把模型拆成 “层”（如卷积层、池化层、全连接层），直接堆叠层即可搭建模型。

- **例子**：LeNet5 模型（手写数字识别）的层结构：

  

  输入（32×32 图像）→ 卷积层 1 → 池化层 1 → 卷积层 2 → 池化层 2 → 全连接层 1 → 全连接层 2 → 输出（10 分类）。

## 六、重点：TensorFlow 入门（初学者必学）

TensorFlow 是目前最流行的框架之一，支持 1.x（静态图）和 2.x（动态图，Eager Execution），初学者建议从 2.x 入手。

### 1. TensorFlow 核心概念

#### （1）张量（Tensor）：数据的载体

- 本质：**n 维数组**，是数据流图中传递的数据。

- 阶数（维度）对应的数据类型：

  | 阶数 | 名称     | 例子                              |
  | ---- | -------- | --------------------------------- |
  | 0    | 标量     | 3.14（单个数值）                  |
  | 1    | 向量     | [1, 2, 3]（一维数组）             |
  | 2    | 矩阵     | [[1,2],[3,4]]（二维数组）         |
  | n    | n 阶张量 | 图像数据（[batch, 高，宽，通道]） |

- 代码例子（TensorFlow 2.x）：

  

  ```python
  import tensorflow as tf
  
  # 0阶张量（标量）
  scalar = tf.constant(3.14)
  # 1阶张量（向量）
  vector = tf.constant([1, 2, 3])
  # 2阶张量（矩阵）
  matrix = tf.constant([[1,2],[3,4]])
  # 3阶张量（模拟单张RGB图像：高2、宽2、通道3）
  image = tf.constant([[[255,0,0], [0,255,0]], [[0,0,255], [255,255,0]]])
  
  print("标量形状：", scalar.shape)  # ()
  print("向量形状：", vector.shape)  # (3,)
  print("矩阵形状：", matrix.shape)  # (2, 2)
  print("图像形状：", image.shape)  # (2, 2, 3)
  ```

  

#### （2）操作（Operation, Op）：计算的最小单元

- 作用：对张量进行计算（如加减乘除、卷积、池化）。
- 常用 Op：
  - 算术运算：`tf.add(a,b)`（加法）、`tf.matmul(a,b)`（矩阵乘法）；
  - 深度学习专用：`tf.nn.conv2d()`（二维卷积）、`tf.nn.max_pool()`（最大池化）、`tf.nn.relu()`（ReLU 激活函数）。

#### （3）变量（Variable）：存储模型参数

- 作用：保存模型的可训练参数（如权重 W、偏置 b），训练时会自动更新。

- 代码例子：

  

  ```python
  # 定义权重（784输入特征 → 200输出特征，随机初始化）
  weights = tf.Variable(tf.random.normal([784, 200], stddev=0.35), name="weights")
  # 定义偏置（200输出特征，初始化为0）
  biases = tf.Variable(tf.zeros([200]), name="biases")
  ```

  

#### （4）会话（Session）：执行计算（仅 TensorFlow 1.x）

- 1.x 中，定义好数据流图后，需要通过会话 “启动” 计算；2.x 中已移除，直接运行代码即可。

- 1.x 代码例子（了解即可）：

  

  ```python
  import tensorflow.compat.v1 as tf
  tf.disable_v2_behavior()  # 禁用2.x特性
  
  # 定义图
  x = tf.constant([[3., 3.]])
  w = tf.constant([[2.], [2.]])
  y = tf.matmul(x, w)  # 矩阵乘法：[3,3] × [2;2] = [12]
  
  # 启动会话执行计算
  with tf.Session() as sess:
      result = sess.run(y)
      print(result)  # 输出：[[12.]]
  ```

  

### 2. TensorFlow 2.x 实战：简单神经网络（手写数字识别简化版）

```python
import tensorflow as tf
from tensorflow.keras import layers, models

# 1. 加载数据集（MNIST手写数字数据集，内置在TensorFlow中）
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
# 预处理：归一化（0-255 → 0-1），增加通道维度（灰度图通道数=1）
x_train = x_train / 255.0
x_test = x_test / 255.0
x_train = x_train[..., tf.newaxis]  # 形状：(60000, 28, 28, 1)
x_test = x_test[..., tf.newaxis]    # 形状：(10000, 28, 28, 1)

# 2. 搭建模型（LeNet5简化版）
model = models.Sequential([
    # 卷积层1：32个3×3卷积核，ReLU激活
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    # 池化层1：2×2最大池化
    layers.MaxPooling2D((2, 2)),
    # 卷积层2：64个3×3卷积核，ReLU激活
    layers.Conv2D(64, (3, 3), activation='relu'),
    # 池化层2：2×2最大池化
    layers.MaxPooling2D((2, 2)),
    # 全连接层1：展平为一维向量，连接128个神经元
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    # 全连接层2：输出10个分类（0-9），Softmax激活
    layers.Dense(10, activation='softmax')
])

# 3. 编译模型（指定优化器、损失函数、评估指标）
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',  # 适用于整数标签
              metrics=['accuracy'])

# 4. 训练模型（5轮迭代，每次用32个样本更新参数）
history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)

# 5. 评估模型（在测试集上验证准确率）
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"测试集准确率：{test_acc:.4f}")  # 通常能达到98%以上
```

### 3. tf.nn 模块：深度学习核心工具

`tf.nn` 是 TensorFlow 中专门用于深度学习的模块，常用功能：

| 功能类别 | 函数               | 作用                           |
| -------- | ------------------ | ------------------------------ |
| 卷积     | `tf.nn.conv2d()`   | 对图像做二维卷积，提取特征     |
| 池化     | `tf.nn.max_pool()` | 最大池化，减少数据量           |
| 激活函数 | `tf.nn.relu()`     | ReLU 激活，解决梯度消失问题    |
|          | `tf.nn.sigmoid()`  | Sigmoid 激活，用于二分类输出   |
| 损失函数 | `tf.nn.l2_loss()`  | L2 正则化，防止过拟合          |
| dropout  | `tf.nn.dropout()`  | 随机丢弃部分神经元，防止过拟合 |

## 七、MindSpore 入门（华为全场景框架）

### 1. 核心特点

- **全场景统一**：一套代码可在手机、服务器、边缘设备上运行；
- **自动优化**：自动并行（拆分模型到多设备）、自动微分（无需手动求导）；
- **易用性**：动态图 / 静态图一键切换，调试方便。

### 2. 简单例子：定义 LeNet5 模型

```python
import mindspore.nn as nn
from mindspore.common.initializer import Normal

class LeNet5(nn.Cell):
    def __init__(self):
        super(LeNet5, self).__init__()
        # 卷积层1：1输入通道→6输出通道，5×5卷积核
        self.conv1 = nn.Conv2d(1, 6, 5, pad_mode='valid', weight_init=Normal(0.02))
        # 卷积层2：6输入通道→16输出通道，5×5卷积核
        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid', weight_init=Normal(0.02))
        # 全连接层1：400输入→120输出
        self.fc1 = nn.Dense(400, 120, weight_init=Normal(0.02))
        # 全连接层2：120输入→84输出
        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))
        # 全连接层3：84输入→10输出（分类）
        self.fc3 = nn.Dense(84, 10, weight_init=Normal(0.02))
        # 激活函数和池化
        self.relu = nn.ReLU()
        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()

    # 前向传播（定义计算流程）
    def construct(self, x):
        x = self.relu(self.conv1(x))
        x = self.max_pool2d(x)
        x = self.relu(self.conv2(x))
        x = self.max_pool2d(x)
        x = self.flatten(x)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

## 八、复习要点

1. **深度学习处理器（DLP）**：
   - 核心计算单元：MAC、向量 / 矩阵单元、激活函数单元；
   - 特点：适配深度学习的 “计算多、并行性强” 需求。
2. **编程框架**：
   - 核心作用：封装底层计算，简化模型搭建；
   - 两类接口：基于数据流图（TensorFlow）、基于层（Caffe）。
3. **TensorFlow 核心**：
   - 张量（n 维数组）、变量（存参数）、操作（计算）；
   - 2.x 动态图：即写即运行，无需会话；
   - `tf.nn`模块：卷积、池化、激活函数等深度学习操作。
4. **分布式训练**：
   - 同步训练：稳定但慢；异步训练：快但可能不稳定。





# 第七章 云计算

## 一、云计算入门：起源与概念

### 1. 为什么会有云计算？（起源）

- **核心原因**：大型 IT 企业（如亚马逊、谷歌）的基础设施是按 “峰值需求” 建设的，但大部分时间里，计算、存储资源都处于闲置状态。这些闲置资源可以封装成公共服务，以低成本、易使用的方式提供给用户，形成了新的商业模式。
- **关键里程碑**：2006 年，亚马逊首次推出**弹性计算云（EC2）**，标志着云计算商业化的开始。
- **通俗比喻**：现在的云计算就像水电煤 —— 是一种 “公用计算资源”，需要时按需取用，按使用量付费，不用自己建设和维护基础设施。
- **常见领域**：电信云（ telecom cloud ）、互联网云（ Internet cloud ）等，资源可动态调整。

### 2. 什么是云计算？（权威定义）

| 定义来源              | 核心内容                                                     | 关键资源                       |
| --------------------- | ------------------------------------------------------------ | ------------------------------ |
| 美国商务部            | 一种 “按需获取网络访问” 的模型，能快速调配共享的可配置计算资源，几乎不需要管理 effort 或与服务提供商交互 | 网络、服务器、存储、应用、服务 |
| Forrester（研究机构） | 抽象化、高可扩展、可管理的计算基础设施池，能托管用户应用，按使用量计费 | 基础设施资源池                 |

- **核心本质**：通过互联网向用户提供软件、硬件、数据和服务，按 “使用量” 和 “服务级别协议（SLA）” 收费。

## 二、云计算的核心特征

初学者可通过 “5 个基本特征 + 7 个补充特征” 快速理解云计算的优势：

### 1. 5 个基本特征（必记）

- **按需自助服务**：用户不用找服务商，自己就能随时申请资源（比如自己开通云服务器）。
- **广泛网络访问**：用普通设备（电脑、手机）通过互联网就能访问云资源，不用局限于特定地点。
- **资源池化**：云服务商把资源集中成 “池”，按用户需求动态分配，用户不用关心资源实际存在哪里（比如你的数据可能存在北京或上海的机房，但你不用管）。
- **快速弹性**：资源能 “快速扩缩容”—— 业务高峰时自动加资源（比如双 11 电商加服务器），低谷时自动减资源，不浪费。
- **计量服务**：按实际使用量收费（比如用 1GB 存储、1 小时计算资源就付对应费用），资源使用可监控、可统计。

### 2. 7 个补充特征（理解优势）

- 可靠性 / 可用性：云服务通常有备份，不容易宕机；
- 成本降低：不用自己买服务器、建机房，减少初期投入；
- 快速上线：新业务不用等基础设施建设，直接用云资源，缩短 “从想法到落地” 的时间；
- 低风险：不用承担硬件折旧、技术更新的风险；
- 虚拟化：底层硬件被抽象成虚拟资源，提高利用率；
- 多租户：多个用户共享同一套基础设施，但数据和资源相互隔离（比如你和邻居用同一栋楼的水电，但各自独立计费）；
- 安全与隐私保障：有专门的安全机制保护数据（后续会细讲）。

## 三、云计算的服务模型（重中之重）

云计算通过 “分层服务” 满足不同用户需求，初学者要分清 **IaaS、PaaS、SaaS** 三大核心模型（还有 BPaaS 作为补充），关键是知道 “自己要负责什么”。

### 1. 四大服务模型对比

| 服务模型 | 英文全称                                        | 核心定义                                                     | 典型例子                                                     | 用户需要管理的内容                                           | 适合人群 / 场景                                |
| -------- | ----------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------------------------- |
| SaaS     | Software as a Service（软件即服务）             | 直接使用云服务商提供的软件，不用安装、维护                   | 谷歌文档（Google Docs）、企业微信、Salesforce CRM（客户管理软件） | 仅需管理 “自己的数据”（如文档内容），不用管底层的服务器、系统 | 普通用户、企业员工（直接用软件办公）           |
| PaaS     | Platform as a Service（平台即服务）             | 提供开发、运行应用的平台（含 OS、中间件、数据库），用户专注写代码 | 谷歌 App Engine、微软 Windows Azure                          | 管理 “自己的代码和数据”，不用管 OS、服务器、中间件           | 程序员、开发团队（开发 APP、网站）             |
| IaaS     | Infrastructure as a Service（基础设施即服务）   | 提供底层基础设施（服务器、存储、网络），用户可装 OS、软件    | 亚马逊 EC2（服务器）、亚马逊 S3（存储）、SQL Azure（数据库） | 管理 “OS、应用、数据”，不用管物理服务器、机房                | 技术团队、企业 IT 部门（需要自主控制底层环境） |
| BPaaS    | Business Process as a Service（业务流程即服务） | 提供完整的业务流程解决方案，比如订单处理、财务报销           | 订单到收款（Order-to-Cash）流程系统                          | 仅需使用业务功能，不用管任何技术细节                         | 企业管理者（需要标准化业务流程）               |

### 2. 一句话总结服务模型

- SaaS：“用软件”—— 像用微信一样，打开就能用；
- PaaS：“搭平台写代码”—— 像在淘宝开店，平台给你工具，你负责上架商品；
- IaaS：“租服务器”—— 像租办公室，你自己装修（装系统、软件）。

## 四、云计算的部署模型（不同场景选不同 “云”）

部署模型决定了 “云资源归谁用、放在哪里”，初学者需掌握 4 种常见模型：

| 部署模型                  | 核心定义                                           | 分类                                                         | 适用场景                                                     | 优点                       |
| ------------------------- | -------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------------- |
| 私有云（Private Cloud）   | 仅为一个组织（如大企业、政府）服务的云，资源不共享 | 1. 本地私有云（服务器在自己机房）；2. 外包私有云（服务器托管在服务商机房） | 对数据安全、隐私要求高的场景（如银行、政府、大型国企）       | 安全性高、可控性强         |
| 公有云（Public Cloud）    | 对公众开放的云，资源由服务商管理，多用户共享       | 无细分，典型如阿里云、AWS、腾讯云                            | 个人用户、中小企业（成本低、不用维护），比如个人用云存储存照片 | 成本低、弹性强、易扩展     |
| 社区云（Community Cloud） | 多个有相同需求的组织（如同一行业的企业）共用的云   | 1. 本地社区云（资源在成员自己的机房）；2. 外包社区云（托管在服务商） | 行业协作场景（如医疗行业共享患者数据平台、教育机构共用教学资源库） | 成本分摊、满足行业特定需求 |
| 混合云（Hybrid Cloud）    | 结合两种及以上部署模型（如私有云 + 公有云）        | 无细分，大部分企业的选择                                     | 需平衡安全和成本的场景（如企业把核心数据放私有云，日常办公用公有云软件） | 灵活度高、兼顾安全与成本   |

- **小知识点**：现在大部分企业用的是**混合云**—— 既保证核心业务的安全（私有云），又利用公有云的弹性降低成本。

## 五、云计算架构：谁参与？有哪些核心组件？

### 1. 5 大核心参与者（角色分工）

| 参与者                     | 核心职责                                                     |
| -------------------------- | ------------------------------------------------------------ |
| 云消费者（Cloud Consumer） | 1. 浏览服务目录，申请需要的服务；2. 与服务商签订 SLA；3. 按使用量付费，使用服务 |
| 云提供者（Cloud Provider） | 1. 提供云资源和服务（如服务器、软件）；2. 负责服务部署、资源调度、安全保障 |
| 云审计者（Cloud Auditor）  | 检查和评估云服务的合规性、安全性、性能（比如验证服务商是否符合数据保护法规） |
| 云经纪人（Cloud Broker）   | 帮消费者找合适的服务：1. 服务中介（对接供需）；2. 服务聚合（整合多个服务商的服务）；3. 服务套利（选性价比最高的服务） |
| 云运营商（Cloud Carrier）  | 提供网络、电信基础设施，保证云服务的网络访问（如中国移动、中国电信） |

### 2. 核心架构组件（资源如何运转）

云计算架构分 “三层”，由 “服务编排” 协调，再加上安全和隐私控制：

1. **服务编排**：负责协调、管理所有资源，分三层：
   - 服务层：对应 SaaS、PaaS、IaaS 三大服务模型；
   - 资源抽象与控制层：将物理资源虚拟化成可调度的资源（如 VM、虚拟存储）；
   - 物理资源层：真实的硬件（服务器、存储设备、网络设备）和机房。
2. **云服务管理**：保障服务正常运行，包括：
   - 业务支持（计费、SLA 管理）；
   - 资源配置（创建、调整 VM）；
   - 可移植性（不同云平台间的资源迁移）。
3. **安全控制**：覆盖所有层级（物理安全→网络安全→系统安全→应用安全），比如防火墙、IDS（入侵检测系统）。
4. **隐私控制**：保护用户个人信息（如账号、支付数据），防止泄露。

## 六、云计算关键技术（初学者需掌握的核心）

### 1. 虚拟化：云计算的 “基石”

- **定义**：创建硬件、OS、存储或网络的 “虚拟版本”，把物理资源抽象成可灵活分配的资源（比如一台物理服务器分成多个虚拟服务器 VM）。
- **核心作用**：让一个物理设备同时跑多个任务，提高资源利用率（比如以前 1 台服务器跑 1 个应用，现在 1 台跑 5 个 VM，每个 VM 跑 1 个应用）。
- **好处**：
  - 简化管理：不用直接维护物理硬件；
  - 硬件独立：VM 可以在不同物理服务器间迁移；
  - 节省成本：减少物理服务器数量；
  - 提高可靠性：一个 VM 故障不影响其他 VM。
- **常见类型**：
  - 硬件虚拟化（最常用，如 VMware、KVM）；
  - 桌面虚拟化（远程使用虚拟桌面，如企业的 VDI）；
  - 存储虚拟化（把多个硬盘整合成一个虚拟存储池）；
  - 网络虚拟化（创建虚拟网络，隔离不同用户的流量）。

### 2. 资源管理与调度：让资源 “物尽其用”

- **核心目标**：高效分配资源，满足用户需求，降低成本。
- **主要内容**：
  1. **VM 管理**：创建、运行、迁移、终止 VM（比如用 OpenNebula 这个开源工具）；
  2. **容量预留**：保证用户在特定时间有足够的 VM（比如电商双 11 前预留 100 台 VM）；
  3. **满足 SLA**：通过策略保证服务质量（比如优先用本地资源减少延迟、平衡负载避免服务器过载）。
- **VM 迁移**：
  - 热迁移（不关机迁移）：用于服务器维护、负载均衡（比如把忙的服务器上的 VM 移到闲的服务器）；
  - 冷迁移（关机迁移）：用于硬件升级。

### 3. 云安全：解决 “数据放在别人那安全吗”

- **覆盖层面**：从物理到人员全链条 —— 物理安全（机房门禁）、网络安全（防火墙）、系统安全（OS 加固）、应用安全（防 SQL 注入）、人员流程（员工权限管理）。
- **常见安全问题**：数据泄露、API 不安全、内部员工恶意操作、服务器故障、账号被盗。
- **关键安全控制方法**：
  - GRC（治理、风险管理、合规）：制定安全政策，评估风险，符合法规（如 GDPR）；
  - 信任保证：服务商要让用户 “放心”—— 比如公开安全流程、在合同中明确数据所有权；
  - 身份与访问控制：验证用户身份（用 SAML 协议），控制资源访问权限（用 XACML 协议）；
  - 数据保护：加密数据（存储和传输时）、用 PDP 协议验证数据完整性（确保数据没被篡改）。

### 4. 数据存储与管理：应对 “大数据” 挑战

- **云存储需求**：要能存海量数据（结构化如 Excel、非结构化如视频），还要支持并行处理。
- **数据一致性模型**（初学者理解 “不同场景下数据的同步速度”）：
  - 强一致性：更新数据后，所有人立刻看到新数据（如银行转账，转完马上到账）；
  - 弱一致性：更新后，部分人可能看到旧数据，需满足特定条件才看到新数据（如社交软件点赞，偶尔延迟显示）；
  - 最终一致性：如果不再更新，最终所有人都会看到新数据（如电商商品库存，高峰期可能有延迟，过会儿同步）。
- **数据安全**：
  - 数据库外包：中小企业把数据库放云服务商那，减少维护成本；
  - 查询完整性：用数字签名保证查询结果正确、完整（比如查账单时，结果没被篡改）；
  - PDP 协议：验证远程存储的数据是否完整（比如你存在云盘的文件没被删除或修改）。

### 5. 编程模型：如何开发云应用？

- **核心需求**：云应用要支持并行处理、可扩展，所以需要专门的编程模型。
- **常见框架**：
  1. **MapReduce（谷歌提出）**：
     - 用途：处理海量数据的并行编程模型（如统计全网搜索关键词）；
     - 核心流程：分两步 ——
       - Map（分解任务）：把大任务拆成多个小任务，分配给不同节点处理；
       - Reduce（合并结果）：把所有小任务的结果合并成最终答案；
     - 例子：统计一本书的单词出现次数 ——Map 负责数每一页的单词，Reduce 负责汇总所有页的结果；
     - 开源实现：Hadoop（Facebook、亚马逊在用）。
  2. **Spark（伯克利提出）**：
     - 改进 MapReduce 的缺点：把中间数据存在内存里（MapReduce 存在硬盘），迭代计算更快（比如反复分析同一批数据）；
     - 支持语言：Python、Java、Scala、SQL、R；
     - 结构：Driver（管理任务）→ Cluster Manager（分配资源）→ Executor（执行任务，有缓存）。

## 七、云联盟与跨云：让云 “互联互通”

- **云联盟（Cloud Federation）**：多个独立的云服务商联合，共享资源（比如 A 公司的云不够用了，用 B 公司的资源），同时保持各自的安全和独立性。
  - 类型：水平联盟（同一层级服务共享，如多个 IaaS 服务商合作）、垂直联盟（不同层级服务整合，如 IaaS+PaaS+SaaS）；
  - 好处：扩大服务范围、提高可靠性、降低成本。
- **跨云（Intercloud）**：未来的方向，像互联网一样，所有云平台都能互联互通（比如你在阿里云的应用能直接用腾讯云的存储），云联盟是 “早期的跨云”。

## 八、从数据中心迁移到云：七步模型

如果企业想把自己的机房（数据中心）迁到云，可按以下步骤：

1. **评估（Assess）**：判断哪些业务适合迁到云，评估成本和风险；
2. **隔离依赖（Isolate）**：理清业务之间的依赖关系（比如 A 系统依赖 B 系统，先迁 B 再迁 A）；
3. **映射（Map）**：把本地资源（如服务器、软件）对应到云资源（如 VM、云数据库）；
4. **重构（Re-architect）**：调整业务架构，适应云环境（比如把单体应用拆成微服务）；
5. **增强功能（Augment）**：添加云特有的功能（如弹性伸缩、云安全服务）；
6. **测试（Test）**：验证迁移后的业务是否正常运行（功能、性能、安全）；
7. **优化（Optimize）**：调整资源配置，降低成本，提升性能（比如关掉闲置的 VM）。

## 九、复习要点（初学者必背）

1. 云计算的核心特征：按需自助、广泛网络访问、资源池化、快速弹性、计量服务；
2. 三大服务模型：SaaS（用软件）、PaaS（搭平台）、IaaS（租基础设施）；
3. 四大部署模型：私有云（安全）、公有云（便宜）、社区云（行业共用）、混合云（灵活）；
4. 关键技术：虚拟化（基础）、资源管理（调度 VM）、云安全（全链条保护）、MapReduce/Spark（大数据处理）；
5. 核心参与者：消费者、提供者、审计者、经纪人、运营商。





# 复习

## 一、整体说明

本笔记基于 “Architecture and Pattern for Parallel Computing（并行计算架构与模式）” 复习 PPT 整理，涵盖 7 个核心章节的知识点总结及考试相关要求，用于备考复习。

## 二、各章节核心知识点

### Chapter 1：原理与基本概念

重点掌握 3 个核心原理，是理解并行计算性能与稳定性的基础：

- **CPU 性能公式**：并行计算中评估 CPU 处理效率的核心公式，用于分析算力瓶颈。
- **可靠性（Reliability）**：衡量系统在规定时间内正常运行的能力，是并行系统稳定工作的关键指标。
- **可用性（Availability）**：衡量系统可被正常访问和使用的概率，与可靠性共同保障并行任务持续执行。

### Chapter 2：高性能内存

围绕 “内存性能优化” 展开，核心包括缓存设计与存储架构两部分：

#### 1. 缓存相关（核心优化方向）

- 基础参数与分析：需掌握**缓存性能参数**（如命中率、失效率等）及**缓存性能分析方法**（评估缓存对整体性能的影响）。
- 核心模型与优化：
  - 模型：**4C 模型**（并行计算中缓存设计的核心参考模型）。
  - 缓存设计优化：**基本缓存设计优化方法**（提升缓存效率的底层设计策略）。
  - 编程层优化：基于缓存技术的编程优化手段，包括：
    - 循环交换（Loop Interchange）
    - 循环融合（Loop Fusion）
    - 数组合并（Array Merging）
    - 矩阵分块（Matrix Blocking）

#### 2. 存储架构

- 并行存储相关技术：**RAID（磁盘阵列）** 及**基于网络的并行存储**（支撑大规模并行数据存储与访问）。

### Chapter 3：特殊指令集

聚焦 “提升数据处理效率的指令级技术”，核心内容：

- **数据的大小端（Big Endian & Little Endian）**：并行系统中数据存储的两种字节序规则，影响跨设备数据交互。
- **移位操作（Shift Operations）**：基础且高效的数据处理指令，常用于并行计算中的快速数值运算。
- **AVX（高级向量扩展）及应用**：高性能向量运算指令集，可显著提升并行数据处理（如矩阵运算、信号处理）的效率。

### Chapter 4：流水线（Pipelining）

并行计算中 “提升指令执行吞吐量” 的核心技术，需掌握性能分析、风险应对与应用技巧：

#### 1. 性能与风险

- 性能分析：**线性流水线的性能分析方法**（评估流水线的吞吐率、延迟等指标）。
- 冒险与应对：
  - 三类流水线冒险：**数据冒险**（数据依赖导致的执行阻塞）、**结构冒险**（硬件资源冲突）、**控制冒险**（分支指令导致的流水线断流）。
  - 对应的**流水线冒险应对措施**（如数据转发、资源复用、分支预测等）。

#### 2. 应用技术

提升流水线效率的编程与设计技巧：

- 循环展开（Loop Unrolling）
- 消除循环携带依赖（Removing Loop-carried Dependence）
- 消除循环中的条件语句（Removing Conditional Statements in Loop）
- 循环倒置（Loop Inverting）
- 分支避免（Branch Avoidance）
- 软件模块流水线（Software Module Pipelining）

### Chapter 5：并行架构与编程

涵盖 “并行硬件架构” 与 “并行程序设计” 两大维度：

#### 1. 并行架构

- 多核与处理器架构：
  - 典型多核架构（常见多核并行硬件的基础结构）。
  - CMP（芯片多处理器）中的**MESI 协议**（缓存一致性协议，保障多核共享数据的一致性）。
  - GPU 架构与**SIMT（单指令多线程）线程执行模型**（GPU 并行计算的核心执行方式）。
- 流控制与集群：
  - SMP（对称多处理器）和 CMP 中**基于缓冲区管理的流控制**（保障数据在多处理器间有序传输）。
  - 集群（Cluster）：大规模并行计算的常用硬件组织形式。

#### 2. 并行程序设计

- **并行程序设计模式**：包含 4 个核心环节：
  1. 寻找并发（Finding Concurrency）：识别任务中可并行执行的部分。
  2. 算法结构设计（Algorithm Structure Design）：设计适配并行的算法框架。
  3. 支持结构（Supporting Structure）：搭建并行程序的支撑组件。
  4. 实现机制（Implementation Mechanism）：落地并行逻辑的技术手段。
- 关键工具：**OpenMP**（跨平台的并行编程 API，用于快速开发共享内存并行程序）。

### Chapter 6：深度学习处理器（DLP）与编程框架

面向 AI 并行计算的专属硬件与软件支撑：

- **DLP 架构与基本组成**：深度学习处理器的硬件结构，适配深度学习任务的并行计算需求。
- **DLP 编程框架**：常用框架包括 TensorFlow、MindSpore（用于开发和部署深度学习并行任务）。
- 核心技术细节：
  - 张量（Tensor）中的常规操作（深度学习并行计算的基本数据单元操作）。
  - DLP 的执行架构（硬件层面的任务执行流程）。
  - 并行编程模型（适配 DLP 的程序设计模型）。

### Chapter 7：云计算

并行计算的重要应用场景，核心包括架构、技术与模型：

- 基础特性：云计算的**基本特征**（如按需分配、弹性扩展等）与**服务模型**（IaaS、PaaS、SaaS 等）。
- 架构：**云计算架构**（从基础设施到应用层的层级结构）。
- 关键技术：**云数据存储与管理**（支撑大规模云端数据的并行存储与高效访问）。
- 编程模型：**MapReduce**与**Spark**（云计算中常用的分布式并行编程模型，用于处理海量数据）。

## 三、考试相关说明

### 1. 考试内容

聚焦 4 个 “基础” 维度，覆盖全章节核心知识点：

- 基本概念
- 基本原理
- 基本计算
- 基本应用

### 2. 携带材料要求

- 仅允许携带**1 张不超过 A4 纸的双面手写材料**（用于辅助记忆）。
- 禁止携带其他任何材料。

### 3. 备考祝福

Good luck for your exam!

