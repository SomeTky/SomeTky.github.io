# stable-baselines3使用基本思路

1. 使用gymnasium构建环境
2. stable-baselines3引入环境训练





# PPO相关参数解释

| `rollout/` | **环境采样阶段**：智能体与环境交互产生的数据 | `ep_len_mean`,`ep_rew_mean`                         |
| ---------- | -------------------------------------------- | --------------------------------------------------- |
| `time/`    | **训练效率监控**：速度与资源消耗             | `fps`,`iterations`,`total_timesteps`,`time_elapsed` |
| `train/`   | **模型更新阶段**：神经网络如何学习           | 所有其他指标（KL、loss、entropy、value_loss 等）    |

### ✅`ep_len_mean` —— 平均episode长度

- **含义**：每个完整回合（episode）中，智能体执行了多少步动作后结束。
- **例子**：
  - CartPole：最大200步，如果平均是195 → 智能体几乎一直保持平衡。
  - 你的任务：3.62 步 → 很可能是“一次性决策任务”（如：走一步就成功/失败）。
- **意义**：
  - 长 = 智能体能持续行动 → 学会了长期策略。
  - 短 = 可能任务简单，或频繁失败。
- **理想值**：
  - 如果环境有最大步数限制（如200），应尽量接近它（说明学得好）。
  - 如果无上限，应稳定在合理范围内（不无限长也不太短）。
- ❗注意：**长度 ≠ 好坏**，关键是奖励是否随长度增长而上升！

### ✅ `ep_rew_mean` —— 平均单 episode 回报（总奖励）

- **含义**：每个 episode 中，智能体获得的所有奖励之和的平均值。

- **关键点**：

  - 这是**衡量智能体表现的终极指标**！
  - 它反映的是：**“智能体到底有没有学会完成目标？”**

- **例子**：

  - CartPole：满分200 → ep_rew_mean=190 表示几乎完美。
  - 你的任务：1.56 → 如果满分是2，则已达到78%；如果满分是10，则才起步。

- **如何判断好坏？**

  - ✅ **上升趋势** = 学习有效
  - ❌ **停滞或下降** = 需要调参或改奖励
  - ⚠️ **突然飙升**（如从1→100）= 奖励函数可能有 bug！

- 💡 **重要提醒**：

  > 一个智能体可以“骗过”奖励函数（比如撞墙得高分），所以必须结合 `ep_len` 和任务逻辑判断奖励是否真实。 

### ✅ `fps` —— Frames Per Second（每秒帧数）

- **含义**：每秒能采集多少个环境步（timestep）。

- **计算方式**：`total_timesteps / time_elapsed`

- **理想值**：

  - > 1000：优秀（CPU/GPU + 多进程采样做得好） 

  - < 500：可能有瓶颈（环境慢、代码低效）

- 你的 1530 → **非常优秀**，说明并行采样高效，硬件利用率高

### ✅ `iterations` —— rollout 迭代次数

- **含义**：PPO 是“交替采样+训练”的算法。每一次 `iteration` = 一次完整的“采样一批数据 + 用这批数据训练几次”。
- **典型流程**：
  1. 采样 2048 步 → 得到一个 rollout
  2. 用这 2048 步数据训练 10 次（n_updates）
  3. → 1 iteration 完成
- 你的 49 次迭代 × ~2048 步 ≈ 100K timesteps，完全匹配。

### ✅ `total_timesteps` —— 总采样步数

- **含义**：智能体与环境交互的总次数（动作执行次数）。
- **意义**：
  - 强化学习是“数据驱动”的，**样本越多，越可能学好**。
  - 通常需要 1M~10M 步才能达到 SOTA 性能（取决于任务复杂度）。
- 你的 100,352 步 → 已经完成“热身”，进入**中期训练阶段**。
- 📊 参考标准：
  - CartPole：100K 足够收敛
  - Mujoco（如 HalfCheetah）：需 3M~10M
  - Atari 游戏：100M+

### ✅ `time_elapsed` —— 总耗时（秒）

- **含义**：从训练开始到现在经过的总时间。
- 用途：估算训练速度、预测收敛所需时间。
- 你的 65 秒跑了 100K 步 → 每 1K 步约 0.65 秒，极快。

## 🎯 核心概念前置：PPO 的两个损失函数

PPO 同时优化两个神经网络：

1. **策略网络（Policy Network）** → 决定“做什么动作”
2. **价值网络（Value Network）** → 估计“当前状态有多好”

训练时最小化两个损失：

- **策略损失**：让好动作更可能被选中（policy gradient）
- **价值损失**：让 V(s) 更接近真实回报（值函数拟合）

同时，为了不让策略更新太激进，引入了：

- **裁剪机制（Clip）**
- **KL 散度约束**

### ✅ `policy_gradient_loss` —— 策略梯度损失

- **含义**：衡量策略更新方向是否正确。负值表示“策略变好了”。

- **公式简化**：

  ```text
  PG_loss = - E[ min( r(θ) * A, clip(r(θ), 1-ε, 1+ε) * A ) ]
  ```

  其中：

  - `r(θ)` 是新旧策略概率比
  - `A` 是优势函数（A = R - V(s)）

- **为什么是负数？**

  - 因为优化器是最小化损失，而我们希望最大化期望回报 → 所以加负号。

- **理想值**：

  - 接近 0，但**不能为正**（正 = 策略变差）
  - 你的 `-0.00668` → **非常健康**，说明策略在缓慢但稳定提升。

- ❗ 注意：这个值**不能单独看**，必须结合 `value_loss` 和 `explained_variance` 判断是否可靠。



### ✅ `value_loss` —— 价值函数损失（均方误差）

- **含义**：价值网络预测的 V(s) 与实际回报（return）之间的**均方误差**。

- **意义**：

  - 如果 value_loss 高 → 价值网络瞎猜 → 优势估计不准 → 策略更新全靠运气
  - 如果 value_loss 低 → V(s) 准确 → 策略更新精准

- **理想值**：

  - < 0.5：优秀

  - 0.5 ~ 2.0：可接受（早期）

  - > 5：严重失效（你的初始版本就是 154！） 

- 你的 `0.512` → **刚过及格线，正在向优秀迈进**，非常棒！



### ✅ `explained_variance` —— 解释方差（价值函数质量的核心指标）

- **含义**：价值函数解释了真实回报多少比例的**变化**。

- **解读**：
  - EV = 0 → V(s) 完全没用，预测等于平均值
  - EV = 1 → V(s) 完美预测所有回报
  - EV = 0.925 → **92.5% 的回报波动都能被状态价值解释** → **顶级水平！**
- **为什么重要？**
  - 如果 EV < 0.5，优势函数 A = return - V(s) 就是噪声 → 策略更新全是瞎蒙
  - 如果 EV > 0.8，说明你训练得非常好，PPO 的“聪明之处”完全发挥出来了
- ✅ 你的 0.925 → **堪比论文级表现**

### ✅ `approx_kl` —— 近似 KL 散度（策略更新幅度）

- **含义**：新策略 π_new 与旧策略 π_old 之间的“差异程度”（用 KL 散度衡量）。

- **为什么需要它？**

  - PPO 的核心思想是：“不要一次改太多！”
  - 如果策略突变太大，会导致训练不稳定甚至崩溃（灾难性遗忘）

- **理想值**：

  - 0.001 ~ 0.02：理想范围（保守更新）

  - > 0.1：更新太激进，可能发散 

  - < 0.001：更新太保守，学习太慢

- 你的 `0.0065` → **完美居中**，说明 PPO 的裁剪机制工作正常，既学得动，又不乱动。



### ✅ `clip_fraction` —— 被裁剪的样本比例

- **含义**：在 PPO 的裁剪机制中，有多少比例的样本因为“概率比太大”被强制截断（clip）了。

- **理想值**：

  - 5% ~ 20%：最佳区间 → 说明大多数样本都在安全区，偶尔有异常值被限制

  - < 5%：更新太保守，可能学得太慢

  - > 30%：策略更新太猛，裁剪频繁，可能不稳定 

- 你的 `0.0734`（7.34%）→ **黄金比例！**

### ✅ `entropy_loss` —— 策略熵损失（探索程度）

- **含义**：策略的随机性（不确定性）。熵越高，策略越“随机”；熵越低，越“确定”。

- **为什么需要它？**
  - 早期：需要高熵 → 多探索
  - 后期：需要低熵 → 精准利用
- **理想趋势**：
  - 初始：-0.4 ~ -0.6（高熵，大量探索）
  - 中期：-0.2 ~ -0.3（适中）
  - 后期：-0.1 或更高（收敛，确定性策略）
- 你的 `-0.216` → **正处于从探索转向利用的过渡期**，非常健康！

### ✅ `loss` —— 总损失（Policy + Value）

- **含义**：PPO 的总损失 = policy_loss + c1 * value_loss
  - 通常 `c1 = 0.5`（权重系数）

- **理想值**：
  - 一般在 **0.05 ~ 1.0** 区间内平稳下降
  - 你的 `0.0948` → **极低**，说明两个子损失都控制得很好
- ❗注意：**不能只看 loss 是否低**，要看它是否**稳定下降**。
  - 如果 loss 忽高忽低 → 不稳定
  - 如果 loss 降为 0 → 可能过拟合或陷入局部最优

### ✅ `clip_range` —— 裁剪范围（超参数）

- **含义**：PPO 中的 ε 参数，控制新旧策略概率比的裁剪边界。
- **公式**：裁剪范围是 `[1-ε, 1+ε]`
- **默认值**：0.2（即 [0.8, 1.2]）
- **作用**：防止策略跳跃过大
- **是否需要调？**
  - 通常固定为 0.2，除非遇到：
    - clip_fraction > 30% → 尝试增大到 0.3
    - learning too slow → 尝试减小到 0.1
- 你的 0.2 → **标准配置，正确使用**